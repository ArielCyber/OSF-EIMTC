<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>EIMTC.models package &mdash; OSF-EIMTC pre-release documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="EIMTC.plugins package" href="EIMTC.plugins.html" />
    <link rel="prev" title="EIMTC.metrics package" href="EIMTC.metrics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> OSF-EIMTC
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="EIMTC.html">EIMTC package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="EIMTC.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="EIMTC.metrics.html">EIMTC.metrics package</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">EIMTC.models package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-EIMTC.models">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="EIMTC.plugins.html">EIMTC.plugins package</a></li>
<li class="toctree-l3"><a class="reference internal" href="EIMTC.preprocessing.html">EIMTC.preprocessing package</a></li>
<li class="toctree-l3"><a class="reference internal" href="EIMTC.selection.html">EIMTC.selection package</a></li>
<li class="toctree-l3"><a class="reference internal" href="EIMTC.stats.html">EIMTC.stats package</a></li>
<li class="toctree-l3"><a class="reference internal" href="EIMTC.tests.html">EIMTC.tests package</a></li>
<li class="toctree-l3"><a class="reference internal" href="EIMTC.third_party.html">EIMTC.third_party package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#eimtc-cli-module">EIMTC.cli module</a></li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#module-EIMTC.extractor">EIMTC.extractor module</a></li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#eimtc-temp-pipelines-module">EIMTC.temp_pipelines module</a></li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#module-EIMTC.tls_tshark_entry">EIMTC.tls_tshark_entry module</a></li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#module-EIMTC.utils">EIMTC.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="EIMTC.html#module-EIMTC">Module contents</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OSF-EIMTC</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="EIMTC.html">EIMTC package</a> &raquo;</li>
      <li>EIMTC.models package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/autodoc/EIMTC.models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="eimtc-models-package">
<h1>EIMTC.models package<a class="headerlink" href="#eimtc-models-package" title="Permalink to this headline"></a></h1>
<div class="section" id="module-EIMTC.models">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-EIMTC.models" title="Permalink to this headline"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.DeepMALRawFlows">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">DeepMALRawFlows</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DeepMALRawFlows" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">keras.engine.training.Model</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DeepMALRawFlows.model">
<span class="sig-name descname"><span class="pre">model</span></span><a class="headerlink" href="#EIMTC.models.DeepMALRawFlows.model" title="Permalink to this definition"></a></dt>
<dd><p>optimizer=’adam’,
loss=tf.keras.losses.CategoricalCrossentropy(),</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DeepMALRawFlows.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DeepMALRawFlows.call" title="Permalink to this definition"></a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<p>Note: This method should not be called directly. It is only meant to be
overridden when subclassing <cite>tf.keras.Model</cite>.
To call a model on an input, always use the <cite>__call__</cite> method,
i.e. <cite>model(inputs)</cite>, which relies on the underlying <cite>call</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensor, or dict/list/tuple of input tensors.</p></li>
<li><p><strong>training</strong> – Boolean or boolean scalar tensor, indicating whether to run
the <cite>Network</cite> in training mode or inference mode.</p></li>
<li><p><strong>mask</strong> – A mask or list of masks. A mask can be
either a tensor or None (no mask).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">CustomDistiller</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">keras.engine.training.Model</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller.call" title="Permalink to this definition"></a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<p>Note: This method should not be called directly. It is only meant to be
overridden when subclassing <cite>tf.keras.Model</cite>.
To call a model on an input, always use the <cite>__call__</cite> method,
i.e. <cite>model(inputs)</cite>, which relies on the underlying <cite>call</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensor, or dict/list/tuple of input tensors.</p></li>
<li><p><strong>training</strong> – Boolean or boolean scalar tensor, indicating whether to run
the <cite>Network</cite> in training mode or inference mode.</p></li>
<li><p><strong>mask</strong> – A mask or list of masks. A mask can be
either a tensor or None (no mask).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller.compile" title="Permalink to this definition"></a></dt>
<dd><p>Configures the model for training.</p>
<p>Example:</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
model.compile(optimizer=tf.keras.optimizer.Adam(learning_rate=1e-3),</p>
<blockquote>
<div><p>loss=tf.keras.losses.BinaryCrossentropy(),
metrics=[tf.keras.metrics.BinaryAccuracy(),</p>
<blockquote>
<div><p>tf.keras.metrics.FalseNegatives()])</p>
</div></blockquote>
</div></blockquote>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> – String (name of optimizer) or optimizer instance. See
<cite>tf.keras.optimizers</cite>.</p></li>
<li><p><strong>loss</strong> – Loss function. Maybe be a string (name of loss function), or
a <cite>tf.keras.losses.Loss</cite> instance. See <cite>tf.keras.losses</cite>. A loss
function is any callable with the signature <cite>loss = fn(y_true,
y_pred)</cite>, where <cite>y_true</cite> are the ground truth values, and
<cite>y_pred</cite> are the model’s predictions.
<cite>y_true</cite> should have shape
<cite>(batch_size, d0, .. dN)</cite> (except in the case of
sparse loss functions such as
sparse categorical crossentropy which expects integer arrays of shape
<cite>(batch_size, d0, .. dN-1)</cite>).
<cite>y_pred</cite> should have shape <cite>(batch_size, d0, .. dN)</cite>.
The loss function should return a float tensor.
If a custom <cite>Loss</cite> instance is
used and reduction is set to <cite>None</cite>, return value has shape
<cite>(batch_size, d0, .. dN-1)</cite> i.e. per-sample or per-timestep loss
values; otherwise, it is a scalar. If the model has multiple outputs,
you can use a different loss on each output by passing a dictionary
or a list of losses. The loss value that will be minimized by the
model will then be the sum of all individual losses, unless
<cite>loss_weights</cite> is specified.</p></li>
<li><p><strong>metrics</strong> – List of metrics to be evaluated by the model during training
and testing. Each of this can be a string (name of a built-in
function), function or a <cite>tf.keras.metrics.Metric</cite> instance. See
<cite>tf.keras.metrics</cite>. Typically you will use <cite>metrics=[‘accuracy’]</cite>. A
function is any callable with the signature <cite>result = fn(y_true,
y_pred)</cite>. To specify different metrics for different outputs of a
multi-output model, you could also pass a dictionary, such as
<cite>metrics={‘output_a’: ‘accuracy’, ‘output_b’: [‘accuracy’, ‘mse’]}</cite>.
You can also pass a list to specify a metric or a list of metrics
for each output, such as <cite>metrics=[[‘accuracy’], [‘accuracy’, ‘mse’]]</cite>
or <cite>metrics=[‘accuracy’, [‘accuracy’, ‘mse’]]</cite>. When you pass the
strings ‘accuracy’ or ‘acc’, we convert this to one of
<cite>tf.keras.metrics.BinaryAccuracy</cite>,
<cite>tf.keras.metrics.CategoricalAccuracy</cite>,
<cite>tf.keras.metrics.SparseCategoricalAccuracy</cite> based on the loss
function used and the model output shape. We do a similar
conversion for the strings ‘crossentropy’ and ‘ce’ as well.</p></li>
<li><p><strong>loss_weights</strong> – <p>Optional list or dictionary specifying scalar coefficients
(Python floats) to weight the loss contributions of different model
outputs. The loss value that will be minimized by the model will then
be the <em>weighted sum</em> of all individual losses, weighted by the
<cite>loss_weights</cite> coefficients.</p>
<blockquote>
<div><dl class="simple">
<dt>If a list, it is expected to have a 1:1 mapping to the model’s</dt><dd><p>outputs. If a dict, it is expected to map output names (strings)
to scalar coefficients.</p>
</dd>
</dl>
</div></blockquote>
</p></li>
<li><p><strong>weighted_metrics</strong> – List of metrics to be evaluated and weighted by
<cite>sample_weight</cite> or <cite>class_weight</cite> during training and testing.</p></li>
<li><p><strong>run_eagerly</strong> – Bool. Defaults to <cite>False</cite>. If <cite>True</cite>, this <cite>Model</cite>’s
logic will not be wrapped in a <cite>tf.function</cite>. Recommended to leave
this as <cite>None</cite> unless your <cite>Model</cite> cannot be run inside a
<cite>tf.function</cite>. <cite>run_eagerly=True</cite> is not supported when using
<cite>tf.distribute.experimental.ParameterServerStrategy</cite>.</p></li>
<li><p><strong>steps_per_execution</strong> – Int. Defaults to 1. The number of batches to
run during each <cite>tf.function</cite> call. Running multiple batches
inside a single <cite>tf.function</cite> call can greatly improve performance
on TPUs or small models with a large Python overhead.
At most, one full epoch will be run each
execution. If a number larger than the size of the epoch is passed,
the execution will be truncated to the size of the epoch.
Note that if <cite>steps_per_execution</cite> is set to <cite>N</cite>,
<cite>Callback.on_batch_begin</cite> and <cite>Callback.on_batch_end</cite> methods
will only be called every <cite>N</cite> batches
(i.e. before/after each <cite>tf.function</cite> execution).</p></li>
<li><p><strong>**kwargs</strong> – Arguments supported for backwards compatibility only.</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – In case of invalid arguments for
    <cite>optimizer</cite>, <cite>loss</cite> or <cite>metrics</cite>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller.fit" title="Permalink to this definition"></a></dt>
<dd><p>Trains the model for a fixed number of epochs (iterations on a dataset).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> – <p>Input data. It could be:
- A Numpy array (or array-like), or a list of arrays</p>
<blockquote>
<div><p>(in case the model has multiple inputs).</p>
</div></blockquote>
<ul>
<li><p>A TensorFlow tensor, or a list of tensors
(in case the model has multiple inputs).</p></li>
<li><p>A dict mapping input names to the corresponding array/tensors,
if the model has named inputs.</p></li>
<li><p>A <cite>tf.data</cite> dataset. Should return a tuple
of either <cite>(inputs, targets)</cite> or
<cite>(inputs, targets, sample_weights)</cite>.</p></li>
<li><p>A generator or <cite>keras.utils.Sequence</cite> returning <cite>(inputs, targets)</cite>
or <cite>(inputs, targets, sample_weights)</cite>.</p></li>
<li><p>A <cite>tf.keras.utils.experimental.DatasetCreator</cite>, which wraps a
callable that takes a single argument of type
<cite>tf.distribute.InputContext</cite>, and returns a <cite>tf.data.Dataset</cite>.
<cite>DatasetCreator</cite> should be used when users prefer to specify the
per-replica batching and sharding logic for the <cite>Dataset</cite>.
See <cite>tf.keras.utils.experimental.DatasetCreator</cite> doc for more
information.</p></li>
</ul>
<p>A more detailed description of unpacking behavior for iterator types
(Dataset, generator, Sequence) is given below. If using
<cite>tf.distribute.experimental.ParameterServerStrategy</cite>, only
<cite>DatasetCreator</cite> type is supported for <cite>x</cite>.</p>
</p></li>
<li><p><strong>y</strong> – Target data. Like the input data <cite>x</cite>,
it could be either Numpy array(s) or TensorFlow tensor(s).
It should be consistent with <cite>x</cite> (you cannot have Numpy inputs and
tensor targets, or inversely). If <cite>x</cite> is a dataset, generator,
or <cite>keras.utils.Sequence</cite> instance, <cite>y</cite> should
not be specified (since targets will be obtained from <cite>x</cite>).</p></li>
<li><p><strong>batch_size</strong> – Integer or <cite>None</cite>.
Number of samples per gradient update.
If unspecified, <cite>batch_size</cite> will default to 32.
Do not specify the <cite>batch_size</cite> if your data is in the
form of datasets, generators, or <cite>keras.utils.Sequence</cite> instances
(since they generate batches).</p></li>
<li><p><strong>epochs</strong> – Integer. Number of epochs to train the model.
An epoch is an iteration over the entire <cite>x</cite> and <cite>y</cite>
data provided.
Note that in conjunction with <cite>initial_epoch</cite>,
<cite>epochs</cite> is to be understood as “final epoch”.
The model is not trained for a number of iterations
given by <cite>epochs</cite>, but merely until the epoch
of index <cite>epochs</cite> is reached.</p></li>
<li><p><strong>verbose</strong> – ‘auto’, 0, 1, or 2. Verbosity mode.
0 = silent, 1 = progress bar, 2 = one line per epoch.
‘auto’ defaults to 1 for most cases, but 2 when used with
<cite>ParameterServerStrategy</cite>. Note that the progress bar is not
particularly useful when logged to a file, so verbose=2 is
recommended when not running interactively (eg, in a production
environment).</p></li>
<li><p><strong>callbacks</strong> – List of <cite>keras.callbacks.Callback</cite> instances.
List of callbacks to apply during training.
See <cite>tf.keras.callbacks</cite>. Note <cite>tf.keras.callbacks.ProgbarLogger</cite>
and <cite>tf.keras.callbacks.History</cite> callbacks are created automatically
and need not be passed into <cite>model.fit</cite>.
<cite>tf.keras.callbacks.ProgbarLogger</cite> is created or not based on
<cite>verbose</cite> argument to <cite>model.fit</cite>.
Callbacks with batch-level calls are currently unsupported with
<cite>tf.distribute.experimental.ParameterServerStrategy</cite>, and users are
advised to implement epoch-level calls instead with an appropriate
<cite>steps_per_epoch</cite> value.</p></li>
<li><p><strong>validation_split</strong> – <dl class="simple">
<dt>Float between 0 and 1.</dt><dd><p>Fraction of the training data to be used as validation data.
The model will set apart this fraction of the training data,
will not train on it, and will evaluate
the loss and any model metrics
on this data at the end of each epoch.
The validation data is selected from the last samples
in the <cite>x</cite> and <cite>y</cite> data provided, before shuffling. This argument is
not supported when <cite>x</cite> is a dataset, generator or</p>
</dd>
<dt><cite>keras.utils.Sequence</cite> instance.</dt><dd><p><cite>validation_split</cite> is not yet supported with
<cite>tf.distribute.experimental.ParameterServerStrategy</cite>.</p>
</dd>
</dl>
</p></li>
<li><p><strong>validation_data</strong> – <p>Data on which to evaluate
the loss and any model metrics at the end of each epoch.
The model will not be trained on this data. Thus, note the fact
that the validation loss of data provided using <cite>validation_split</cite>
or <cite>validation_data</cite> is not affected by regularization layers like
noise and dropout.
<cite>validation_data</cite> will override <cite>validation_split</cite>.
<cite>validation_data</cite> could be:</p>
<blockquote>
<div><ul>
<li><p>A tuple <cite>(x_val, y_val)</cite> of Numpy arrays or tensors.</p></li>
<li><p>A tuple <cite>(x_val, y_val, val_sample_weights)</cite> of NumPy arrays.</p></li>
<li><p>A <cite>tf.data.Dataset</cite>.</p></li>
<li><p>A Python generator or <cite>keras.utils.Sequence</cite> returning</p></li>
</ul>
<p><cite>(inputs, targets)</cite> or <cite>(inputs, targets, sample_weights)</cite>.</p>
</div></blockquote>
<p><cite>validation_data</cite> is not yet supported with
<cite>tf.distribute.experimental.ParameterServerStrategy</cite>.</p>
</p></li>
<li><p><strong>shuffle</strong> – Boolean (whether to shuffle the training data
before each epoch) or str (for ‘batch’). This argument is ignored
when <cite>x</cite> is a generator or an object of tf.data.Dataset.
‘batch’ is a special option for dealing
with the limitations of HDF5 data; it shuffles in batch-sized
chunks. Has no effect when <cite>steps_per_epoch</cite> is not <cite>None</cite>.</p></li>
<li><p><strong>class_weight</strong> – Optional dictionary mapping class indices (integers)
to a weight (float) value, used for weighting the loss function
(during training only).
This can be useful to tell the model to
“pay more attention” to samples from
an under-represented class.</p></li>
<li><p><strong>sample_weight</strong> – <dl class="simple">
<dt>Optional Numpy array of weights for</dt><dd><p>the training samples, used for weighting the loss function
(during training only). You can either pass a flat (1D)
Numpy array with the same length as the input samples
(1:1 mapping between weights and samples),
or in the case of temporal data,
you can pass a 2D array with shape
<cite>(samples, sequence_length)</cite>,
to apply a different weight to every timestep of every sample. This
argument is not supported when <cite>x</cite> is a dataset, generator, or</p>
</dd>
<dt><cite>keras.utils.Sequence</cite> instance, instead provide the sample_weights</dt><dd><p>as the third element of <cite>x</cite>.</p>
</dd>
</dl>
</p></li>
<li><p><strong>initial_epoch</strong> – Integer.
Epoch at which to start training
(useful for resuming a previous training run).</p></li>
<li><p><strong>steps_per_epoch</strong> – <p>Integer or <cite>None</cite>.
Total number of steps (batches of samples)
before declaring one epoch finished and starting the
next epoch. When training with input tensors such as
TensorFlow data tensors, the default <cite>None</cite> is equal to
the number of samples in your dataset divided by
the batch size, or 1 if that cannot be determined. If x is a
<cite>tf.data</cite> dataset, and ‘steps_per_epoch’
is None, the epoch will run until the input dataset is exhausted.
When passing an infinitely repeating dataset, you must specify the
<cite>steps_per_epoch</cite> argument. If <cite>steps_per_epoch=-1</cite> the training
will run indefinitely with an infinitely repeating dataset.
This argument is not supported with array inputs.
When using <cite>tf.distribute.experimental.ParameterServerStrategy</cite>:</p>
<blockquote>
<div><ul>
<li><p><cite>steps_per_epoch=None</cite> is not supported.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>validation_steps</strong> – Only relevant if <cite>validation_data</cite> is provided and
is a <cite>tf.data</cite> dataset. Total number of steps (batches of
samples) to draw before stopping when performing validation
at the end of every epoch. If ‘validation_steps’ is None, validation
will run until the <cite>validation_data</cite> dataset is exhausted. In the
case of an infinitely repeated dataset, it will run into an
infinite loop. If ‘validation_steps’ is specified and only part of
the dataset will be consumed, the evaluation will start from the
beginning of the dataset at each epoch. This ensures that the same
validation samples are used every time.</p></li>
<li><p><strong>validation_batch_size</strong> – Integer or <cite>None</cite>.
Number of samples per validation batch.
If unspecified, will default to <cite>batch_size</cite>.
Do not specify the <cite>validation_batch_size</cite> if your data is in the
form of datasets, generators, or <cite>keras.utils.Sequence</cite> instances
(since they generate batches).</p></li>
<li><p><strong>validation_freq</strong> – Only relevant if validation data is provided. Integer
or <cite>collections.abc.Container</cite> instance (e.g. list, tuple, etc.).
If an integer, specifies how many training epochs to run before a
new validation run is performed, e.g. <cite>validation_freq=2</cite> runs
validation every 2 epochs. If a Container, specifies the epochs on
which to run validation, e.g. <cite>validation_freq=[1, 2, 10]</cite> runs
validation at the end of the 1st, 2nd, and 10th epochs.</p></li>
<li><p><strong>max_queue_size</strong> – Integer. Used for generator or <cite>keras.utils.Sequence</cite>
input only. Maximum size for the generator queue.
If unspecified, <cite>max_queue_size</cite> will default to 10.</p></li>
<li><p><strong>workers</strong> – Integer. Used for generator or <cite>keras.utils.Sequence</cite> input
only. Maximum number of processes to spin up
when using process-based threading. If unspecified, <cite>workers</cite>
will default to 1.</p></li>
<li><p><strong>use_multiprocessing</strong> – Boolean. Used for generator or
<cite>keras.utils.Sequence</cite> input only. If <cite>True</cite>, use process-based
threading. If unspecified, <cite>use_multiprocessing</cite> will default to
<cite>False</cite>. Note that because this implementation relies on
multiprocessing, you should not pass non-picklable arguments to
the generator as they can’t be passed easily to children processes.</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Unpacking behavior for iterator-like inputs:</dt><dd><blockquote>
<div><p>A common pattern is to pass a tf.data.Dataset, generator, or</p>
</div></blockquote>
<p>tf.keras.utils.Sequence to the <cite>x</cite> argument of fit, which will in fact
yield not only features (x) but optionally targets (y) and sample weights.
Keras requires that the output of such iterator-likes be unambiguous. The
iterator should return a tuple of length 1, 2, or 3, where the optional
second and third elements will be used for y and sample_weight
respectively. Any other type provided will be wrapped in a length one
tuple, effectively treating everything as ‘x’. When yielding dicts, they
should still adhere to the top-level tuple structure.
e.g. <cite>({“x0”: x0, “x1”: x1}, y)</cite>. Keras will not attempt to separate
features, targets, and weights from the keys of a single dict.</p>
<blockquote>
<div><p>A notable unsupported data type is the namedtuple. The reason is that</p>
</div></blockquote>
<p>it behaves like both an ordered datatype (tuple) and a mapping
datatype (dict). So given a namedtuple of the form:</p>
<blockquote>
<div><p><cite>namedtuple(“example_tuple”, [“y”, “x”])</cite></p>
</div></blockquote>
<p>it is ambiguous whether to reverse the order of the elements when
interpreting the value. Even worse is a tuple of the form:</p>
<blockquote>
<div><p><cite>namedtuple(“other_tuple”, [“x”, “y”, “z”])</cite></p>
</div></blockquote>
<p>where it is unclear if the tuple was intended to be unpacked into x, y,
and sample_weight or passed through as a single element to <cite>x</cite>. As a
result the data processing code will simply raise a ValueError if it
encounters a namedtuple. (Along with instructions to remedy the issue.)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A <cite>History</cite> object. Its <cite>History.history</cite> attribute is
a record of training loss values and metrics values
at successive epochs, as well as validation loss values
and validation metrics values (if applicable).</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>RuntimeError</strong> – <ol class="arabic simple">
<li><p>If the model was never compiled or,</p></li>
</ol>
</p></li>
<li><p><strong>2. If model.fit is  wrapped in tf.function.</strong> – </p></li>
<li><p><strong>ValueError</strong> – In case of mismatch between the provided input data
    and what the model expects or when the input data is empty.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller.get_model_for_pretraining">
<span class="sig-name descname"><span class="pre">get_model_for_pretraining</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller.get_model_for_pretraining" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller.freeze_for_finetuning">
<span class="sig-name descname"><span class="pre">freeze_for_finetuning</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller.freeze_for_finetuning" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CustomDistiller.unfreeze_for_finetuning">
<span class="sig-name descname"><span class="pre">unfreeze_for_finetuning</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CustomDistiller.unfreeze_for_finetuning" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.M1CNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">M1CNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.M1CNN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">keras.engine.training.Model</span></code></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.M1CNN.model">
<span class="sig-name descname"><span class="pre">model</span></span><a class="headerlink" href="#EIMTC.models.M1CNN.model" title="Permalink to this definition"></a></dt>
<dd><p>optimizer=’adam’,
loss=tf.keras.losses.CategoricalCrossentropy(),</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.M1CNN.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">training</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.M1CNN.call" title="Permalink to this definition"></a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<p>Note: This method should not be called directly. It is only meant to be
overridden when subclassing <cite>tf.keras.Model</cite>.
To call a model on an input, always use the <cite>__call__</cite> method,
i.e. <cite>model(inputs)</cite>, which relies on the underlying <cite>call</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensor, or dict/list/tuple of input tensors.</p></li>
<li><p><strong>training</strong> – Boolean or boolean scalar tensor, indicating whether to run
the <cite>Network</cite> in training mode or inference mode.</p></li>
<li><p><strong>mask</strong> – A mask or list of masks. A mask can be
either a tensor or None (no mask).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.Distiller">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">Distiller</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.Distiller" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#EIMTC.models.CustomDistiller" title="EIMTC.models._custom_distiller.CustomDistiller"><code class="xref py py-class docutils literal notranslate"><span class="pre">EIMTC.models._custom_distiller.CustomDistiller</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.MalDist">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">MalDist</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.MalDist" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#EIMTC.models.CustomDistiller" title="EIMTC.models._custom_distiller.CustomDistiller"><code class="xref py py-class docutils literal notranslate"><span class="pre">EIMTC.models._custom_distiller.CustomDistiller</span></code></a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.GraphDApp">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">GraphDApp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GraphDApp" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">keras.engine.training.Model</span></code></p>
<p># GraphDApp Model
## Parameters</p>
<ul class="simple">
<li><p><cite>n_classes</cite> (int): Number of possible classes an input can be classified into (#units in final dense layer).</p></li>
<li><p><cite>mlplayer_units</cite> (int): Number of units in each single-MLP layer.</p></li>
</ul>
<p>## Input
### Per sample
- Graph adjacancy matrix</p>
<blockquote>
<div><ul class="simple">
<li><p>Shape: (n_nodes, n_nodes)</p></li>
</ul>
</div></blockquote>
<ul class="simple">
<li><dl class="simple">
<dt>Node features</dt><dd><ul>
<li><p>Shape: (n_nodes, n_features_per_node)</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>### Final input shape
[(n_samples, n_nodes, n, nodes), (n_samples, n_nodes, n_features_per_node)]</p>
<p>## Paper</p>
<p>“Accurate Decentralized Application Identification via Encrypted Traffic Analysis Using Graph Neural Networks,”</p>
<p>### By</p>
<ul class="simple">
<li><p>Meng Shen.</p></li>
<li><p>Jinpeng Zhang.</p></li>
<li><p>Liehuang Zhu.</p></li>
<li><p>Ke Xu.</p></li>
<li><p>Xiaojiang Du.</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GraphDApp.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GraphDApp.call" title="Permalink to this definition"></a></dt>
<dd><p>Calls the model on new inputs.</p>
<p>In this case <cite>call</cite> just reapplies
all ops in the graph to the new inputs
(e.g. build a new computational graph from the provided inputs).</p>
<p>Note: This method should not be called directly. It is only meant to be
overridden when subclassing <cite>tf.keras.Model</cite>.
To call a model on an input, always use the <cite>__call__</cite> method,
i.e. <cite>model(inputs)</cite>, which relies on the underlying <cite>call</cite> method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> – Input tensor, or dict/list/tuple of input tensors.</p></li>
<li><p><strong>training</strong> – Boolean or boolean scalar tensor, indicating whether to run
the <cite>Network</cite> in training mode or inference mode.</p></li>
<li><p><strong>mask</strong> – A mask or list of masks. A mask can be
either a tensor or None (no mask).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor if there is a single output, or
a list of tensors if there are more than one outputs.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.ChainedGNN">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">ChainedGNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ChainedGNN" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">keras.engine.training.Model</span></code></p>
<p>## Paper</p>
<p>“CGNN: Traffic Classification with Graph Neural Network”
### By
- Bo Pang.
- Yongquan Fu.
- Siyuan Ren.
- Ye Wang.
- Qing Liao.
- Yan Jia.</p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.ChainedGNN.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ChainedGNN.call" title="Permalink to this definition"></a></dt>
<dd><p>x = [A, F] or F. Depends on use_precomputed_norm_adj_mat</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVC">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">LinearSVC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_hinge'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ovr'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.LinearSVC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.SparseCoefMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Linear Support Vector Classification.</p>
<p>Similar to SVC with parameter kernel=’linear’, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input and the multiclass support
is handled according to a one-vs-the-rest scheme.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>penalty</strong> (<em>{'l1'</em><em>, </em><em>'l2'}</em><em>, </em><em>default='l2'</em>) – Specifies the norm used in the penalization. The ‘l2’
penalty is the standard used in SVC. The ‘l1’ leads to <code class="docutils literal notranslate"><span class="pre">coef_</span></code>
vectors that are sparse.</p></li>
<li><p><strong>loss</strong> (<em>{'hinge'</em><em>, </em><em>'squared_hinge'}</em><em>, </em><em>default='squared_hinge'</em>) – Specifies the loss function. ‘hinge’ is the standard SVM loss
(used e.g. by the SVC class) while ‘squared_hinge’ is the
square of the hinge loss. The combination of <code class="docutils literal notranslate"><span class="pre">penalty='l1'</span></code>
and <code class="docutils literal notranslate"><span class="pre">loss='hinge'</span></code> is not supported.</p></li>
<li><p><strong>dual</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Tolerance for stopping criteria.</p></li>
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Regularization parameter. The strength of the regularization is
inversely proportional to C. Must be strictly positive.</p></li>
<li><p><strong>multi_class</strong> (<em>{'ovr'</em><em>, </em><em>'crammer_singer'}</em><em>, </em><em>default='ovr'</em>) – Determines the multi-class strategy if <cite>y</cite> contains more than
two classes.
<code class="docutils literal notranslate"><span class="pre">&quot;ovr&quot;</span></code> trains n_classes one-vs-rest classifiers, while
<code class="docutils literal notranslate"><span class="pre">&quot;crammer_singer&quot;</span></code> optimizes a joint objective over all classes.
While <cite>crammer_singer</cite> is interesting from a theoretical perspective
as it is consistent, it is seldom used in practice as it rarely leads
to better accuracy and is more expensive to compute.
If <code class="docutils literal notranslate"><span class="pre">&quot;crammer_singer&quot;</span></code> is chosen, the options loss, penalty and dual
will be ignored.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be already centered).</p></li>
<li><p><strong>intercept_scaling</strong> (<em>float</em><em>, </em><em>default=1</em>) – When self.fit_intercept is True, instance vector x becomes
<code class="docutils literal notranslate"><span class="pre">[x,</span> <span class="pre">self.intercept_scaling]</span></code>,
i.e. a “synthetic” feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em> or </em><em>'balanced'</em><em>, </em><em>default=None</em>) – Set the parameter C of class i to <code class="docutils literal notranslate"><span class="pre">class_weight[i]*C</span></code> for
SVC. If not given, all classes are supposed to have
weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the pseudo random number generation for shuffling the data for
the dual coordinate descent (if <code class="docutils literal notranslate"><span class="pre">dual=True</span></code>). When <code class="docutils literal notranslate"><span class="pre">dual=False</span></code> the
underlying implementation of <a class="reference internal" href="#EIMTC.models.LinearSVC" title="EIMTC.models.LinearSVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a> is not random and
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> has no effect on the results.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations to be run.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVC.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVC.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem).</p>
<p><code class="docutils literal notranslate"><span class="pre">coef_</span></code> is a readonly property derived from <code class="docutils literal notranslate"><span class="pre">raw_coef_</span></code> that
follows the internal memory layout of liblinear.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVC.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVC.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,) if n_classes == 2 else (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVC.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVC.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The unique classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVC.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVC.n_iter_" title="Permalink to this definition"></a></dt>
<dd><p>Maximum number of iterations run across all classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.SVC" title="EIMTC.models.SVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SVC</span></code></a></dt><dd><p>Implementation of Support Vector Machine classifier using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does. Furthermore SVC multi-class mode is implemented using one vs one scheme while LinearSVC uses one vs the rest. It is possible to implement one vs the rest with SVC by using the <code class="xref py py-class docutils literal notranslate"><span class="pre">OneVsRestClassifier</span></code> wrapper. Finally SVC can fit dense data without memory copy if the input is C-contiguous. Sparse data will still incur memory copy though.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDClassifier</span></code></dt><dd><p>SGDClassifier can optimize the same cost function as LinearSVC by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon
to have slightly different results for the same input data. If
that happens, try with a smaller <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter.</p>
<p>The underlying implementation, liblinear, uses a sparse internal
representation for the data that will incur a memory copy.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See <span class="xref std std-ref">differences from liblinear</span>
in the narrative documentation.</p>
<p class="rubric">References</p>
<p><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR: A Library for Large Linear Classification</a></p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                    <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;linearsvc&#39;, LinearSVC(random_state=0, tol=1e-05))])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linearsvc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.141...   0.526... 0.679... 0.493...]]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linearsvc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.1693...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVC.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.LinearSVC.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples in the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target vector relative to X.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Array of weights that are assigned to individual
samples. If not provided,
then each sample is given unit weight.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – An instance of the estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVR">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">LinearSVR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'epsilon_insensitive'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.LinearSVR" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Linear Support Vector Regression.</p>
<p>Similar to SVR with parameter kernel=’linear’, but implemented in terms of
liblinear rather than libsvm, so it has more flexibility in the choice of
penalties and loss functions and should scale better to large numbers of
samples.</p>
<p>This class supports both dense and sparse input.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.16.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – Epsilon parameter in the epsilon-insensitive loss function. Note
that the value of this parameter depends on the scale of the target
variable y. If unsure, set <code class="docutils literal notranslate"><span class="pre">epsilon=0</span></code>.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Tolerance for stopping criteria.</p></li>
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Regularization parameter. The strength of the regularization is
inversely proportional to C. Must be strictly positive.</p></li>
<li><p><strong>loss</strong> (<em>{'epsilon_insensitive'</em><em>, </em><em>'squared_epsilon_insensitive'}</em><em>,             </em><em>default='epsilon_insensitive'</em>) – Specifies the loss function. The epsilon-insensitive loss
(standard SVR) is the L1 loss, while the squared epsilon-insensitive
loss (‘squared_epsilon_insensitive’) is the L2 loss.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be already centered).</p></li>
<li><p><strong>intercept_scaling</strong> (<em>float</em><em>, </em><em>default=1.</em>) – When self.fit_intercept is True, instance vector x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equals to
intercept_scaling is appended to the instance vector.
The intercept becomes intercept_scaling * synthetic feature weight
Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</p></li>
<li><p><strong>dual</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Select the algorithm to either solve the dual or primal
optimization problem. Prefer dual=False when n_samples &gt; n_features.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in liblinear that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the pseudo random number generation for shuffling the data.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations to be run.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVR.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVR.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem).</p>
<p><cite>coef_</cite> is a readonly property derived from <cite>raw_coef_</cite> that
follows the internal memory layout of liblinear.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features) if n_classes == 2             else (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVR.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVR.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1) if n_classes == 2 else (n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVR.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#EIMTC.models.LinearSVR.n_iter_" title="Permalink to this definition"></a></dt>
<dd><p>Maximum number of iterations run across all classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                     <span class="n">LinearSVR</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;linearsvr&#39;, LinearSVR(random_state=0, tol=1e-05))])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linearsvr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[18.582... 27.023... 44.357... 64.522...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;linearsvr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[-4...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-2.384...]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.LinearSVC" title="EIMTC.models.LinearSVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearSVC</span></code></a></dt><dd><p>Implementation of Support Vector Machine classifier using the same library as this class (liblinear).</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.SVR" title="EIMTC.models.SVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SVR</span></code></a></dt><dd><p>Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDRegressor</span></code></dt><dd><p>SGDRegressor can optimize the same cost function as LinearSVR by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes.</p>
</dd>
</dl>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.LinearSVR.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.LinearSVR.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples in the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target vector relative to X</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Array of weights that are assigned to individual
samples. If not provided,
then each sample is given unit weight.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – An instance of the estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">NuSVC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'scale'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrinking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decision_function_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ovr'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">break_ties</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NuSVC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm._base.BaseSVC</span></code></p>
<p>Nu-Support Vector Classification.</p>
<p>Similar to SVC but uses a parameter to control the number of support
vectors.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nu</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – An upper bound on the fraction of margin errors (see <span class="xref std std-ref">User Guide</span>) and a lower bound of the fraction of support vectors.
Should be in the interval (0, 1].</p></li>
<li><p><strong>kernel</strong> (<em>{'linear'</em><em>, </em><em>'poly'</em><em>, </em><em>'rbf'</em><em>, </em><em>'sigmoid'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='rbf'</em>) – Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</p></li>
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>default=3</em>) – Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p></li>
<li><p><strong>gamma</strong> (<em>{'scale'</em><em>, </em><em>'auto'}</em><em> or </em><em>float</em><em>, </em><em>default='scale'</em>) – <p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<ul>
<li><p>if <code class="docutils literal notranslate"><span class="pre">gamma='scale'</span></code> (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,</p></li>
<li><p>if ‘auto’, uses 1 / n_features.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">gamma</span></code> changed from ‘auto’ to ‘scale’.</p>
</div>
</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p></li>
<li><p><strong>shrinking</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use the shrinking heuristic.
See the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>probability</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, will slow down that method as it internally uses
5-fold cross-validation, and <cite>predict_proba</cite> may be inconsistent with
<cite>predict</cite>. Read more in the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Tolerance for stopping criterion.</p></li>
<li><p><strong>cache_size</strong> (<em>float</em><em>, </em><em>default=200</em>) – Specify the size of the kernel cache (in MB).</p></li>
<li><p><strong>class_weight</strong> (<em>{dict</em><em>, </em><em>'balanced'}</em><em>, </em><em>default=None</em>) – Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one. The “balanced” mode uses the values of y to automatically
adjust weights inversely proportional to class frequencies as
<code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=-1</em>) – Hard limit on iterations within solver, or -1 for no limit.</p></li>
<li><p><strong>decision_function_shape</strong> (<em>{'ovo'</em><em>, </em><em>'ovr'}</em><em>, </em><em>default='ovr'</em>) – <p>Whether to return a one-vs-rest (‘ovr’) decision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (‘ovo’) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
(‘ovo’) is always used as multi-class strategy. The parameter is
ignored for binary classification.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>decision_function_shape is ‘ovr’ by default.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>decision_function_shape=’ovr’</em> is recommended.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=’ovo’ and None</em>.</p>
</div>
</p></li>
<li><p><strong>break_ties</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If true, <code class="docutils literal notranslate"><span class="pre">decision_function_shape='ovr'</span></code>, and number of classes &gt; 2,
<span class="xref std std-term">predict</span> will break ties according to the confidence values of
<span class="xref std std-term">decision_function</span>; otherwise the first class among the tied
classes is returned. Please note that breaking ties comes at a
relatively high computational cost compared to a simple predict.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the pseudo random number generation for shuffling the data for
probability estimates. Ignored when <cite>probability</cite> is False.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.class_weight_">
<span class="sig-name descname"><span class="pre">class_weight_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.class_weight_" title="Permalink to this definition"></a></dt>
<dd><p>Multipliers of parameter C of each class.
Computed based on the <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The unique classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes -1) / 2, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.dual_coef_">
<span class="sig-name descname"><span class="pre">dual_coef_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.dual_coef_" title="Permalink to this definition"></a></dt>
<dd><p>Dual coefficients of the support vector in the decision
function (see <span class="xref std std-ref">sgd_mathematical_formulation</span>), multiplied by
their targets.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the <span class="xref std std-ref">multi-class section of the User Guide</span> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes - 1, n_SV)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.fit_status_">
<span class="sig-name descname"><span class="pre">fit_status_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.fit_status_" title="Permalink to this definition"></a></dt>
<dd><p>0 if correctly fitted, 1 if the algorithm did not converge.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.support_">
<span class="sig-name descname"><span class="pre">support_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.support_" title="Permalink to this definition"></a></dt>
<dd><p>Indices of support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.support_vectors_">
<span class="sig-name descname"><span class="pre">support_vectors_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.support_vectors_" title="Permalink to this definition"></a></dt>
<dd><p>Support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.n_support_">
<span class="sig-name descname"><span class="pre">n_support_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.n_support_" title="Permalink to this definition"></a></dt>
<dd><p>Number of support vectors for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,), dtype=int32</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">fit_status_</span></span><a class="headerlink" href="#id0" title="Permalink to this definition"></a></dt>
<dd><p>0 if correctly fitted, 1 if the algorithm did not converge.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.probA_">
<span class="sig-name descname"><span class="pre">probA_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.probA_" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.probB_">
<span class="sig-name descname"><span class="pre">probB_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.probB_" title="Permalink to this definition"></a></dt>
<dd><p>If <cite>probability=True</cite>, it corresponds to the parameters learned in
Platt scaling to produce probability estimates from decision values.
If <cite>probability=False</cite>, it’s an empty array. Platt scaling uses the
logistic function
<code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(decision_value</span> <span class="pre">*</span> <span class="pre">probA_</span> <span class="pre">+</span> <span class="pre">probB_))</span></code>
where <code class="docutils literal notranslate"><span class="pre">probA_</span></code> and <code class="docutils literal notranslate"><span class="pre">probB_</span></code> are learned from the dataset <a href="#id72"><span class="problematic" id="id9">[2]_</span></a>. For
more information on the multiclass case and training procedure see
section 8 of <a href="#id73"><span class="problematic" id="id10">[1]_</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVC.shape_fit_">
<span class="sig-name descname"><span class="pre">shape_fit_</span></span><a class="headerlink" href="#EIMTC.models.NuSVC.shape_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Array dimensions of training vector <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int of shape (n_dimensions_of_X,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">NuSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">NuSVC</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()), (&#39;nusvc&#39;, NuSVC())])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.SVC" title="EIMTC.models.SVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SVC</span></code></a></dt><dd><p>Support Vector Machine for classification using libsvm.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.LinearSVC" title="EIMTC.models.LinearSVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearSVC</span></code></a></dt><dd><p>Scalable linear Support Vector Machine for classification using liblinear.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
</dd>
<dt class="label" id="id12"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639">Platt, John (1999). “Probabilistic outputs for support vector
machines and comparison to regularizedlikelihood methods.”</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">NuSVR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'scale'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrinking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NuSVR" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm._base.BaseLibSVM</span></code></p>
<p>Nu Support Vector Regression.</p>
<p>Similar to NuSVC, for regression, uses a parameter nu to control
the number of support vectors. However, unlike NuSVC, where nu
replaces C, here nu replaces the parameter epsilon of epsilon-SVR.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nu</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – An upper bound on the fraction of training errors and a lower bound of
the fraction of support vectors. Should be in the interval (0, 1].  By
default 0.5 will be taken.</p></li>
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Penalty parameter C of the error term.</p></li>
<li><p><strong>kernel</strong> (<em>{'linear'</em><em>, </em><em>'poly'</em><em>, </em><em>'rbf'</em><em>, </em><em>'sigmoid'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='rbf'</em>) – Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</p></li>
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>default=3</em>) – Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p></li>
<li><p><strong>gamma</strong> (<em>{'scale'</em><em>, </em><em>'auto'}</em><em> or </em><em>float</em><em>, </em><em>default='scale'</em>) – <p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<ul>
<li><p>if <code class="docutils literal notranslate"><span class="pre">gamma='scale'</span></code> (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,</p></li>
<li><p>if ‘auto’, uses 1 / n_features.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">gamma</span></code> changed from ‘auto’ to ‘scale’.</p>
</div>
</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p></li>
<li><p><strong>shrinking</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use the shrinking heuristic.
See the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Tolerance for stopping criterion.</p></li>
<li><p><strong>cache_size</strong> (<em>float</em><em>, </em><em>default=200</em>) – Specify the size of the kernel cache (in MB).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=-1</em>) – Hard limit on iterations within solver, or -1 for no limit.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.class_weight_">
<span class="sig-name descname"><span class="pre">class_weight_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.class_weight_" title="Permalink to this definition"></a></dt>
<dd><p>Multipliers of parameter C for each class.
Computed based on the <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.dual_coef_">
<span class="sig-name descname"><span class="pre">dual_coef_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.dual_coef_" title="Permalink to this definition"></a></dt>
<dd><p>Coefficients of the support vector in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_SV)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.fit_status_">
<span class="sig-name descname"><span class="pre">fit_status_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.fit_status_" title="Permalink to this definition"></a></dt>
<dd><p>0 if correctly fitted, 1 otherwise (will raise warning)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.n_support_">
<span class="sig-name descname"><span class="pre">n_support_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.n_support_" title="Permalink to this definition"></a></dt>
<dd><p>Number of support vectors for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,), dtype=int32</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.shape_fit_">
<span class="sig-name descname"><span class="pre">shape_fit_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.shape_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Array dimensions of training vector <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int of shape (n_dimensions_of_X,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.support_">
<span class="sig-name descname"><span class="pre">support_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.support_" title="Permalink to this definition"></a></dt>
<dd><p>Indices of support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NuSVR.support_vectors_">
<span class="sig-name descname"><span class="pre">support_vectors_</span></span><a class="headerlink" href="#EIMTC.models.NuSVR.support_vectors_" title="Permalink to this definition"></a></dt>
<dd><p>Support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV, n_features)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">NuSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">NuSVR</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="mf">0.1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;nusvr&#39;, NuSVR(nu=0.1))])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.NuSVC" title="EIMTC.models.NuSVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NuSVC</span></code></a></dt><dd><p>Support Vector Machine for classification implemented with libsvm with a parameter to control the number of support vectors.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.SVR" title="EIMTC.models.SVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SVR</span></code></a></dt><dd><p>Epsilon Support Vector Machine for regression implemented with libsvm.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id13"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
</dd>
<dt class="label" id="id15"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639">Platt, John (1999). “Probabilistic outputs for support vector
machines and comparison to regularizedlikelihood methods.”</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">OneClassSVM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'scale'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrinking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.OneClassSVM" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.OutlierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm._base.BaseLibSVM</span></code></p>
<p>Unsupervised Outlier Detection.</p>
<p>Estimate the support of a high-dimensional distribution.</p>
<p>The implementation is based on libsvm.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel</strong> (<em>{'linear'</em><em>, </em><em>'poly'</em><em>, </em><em>'rbf'</em><em>, </em><em>'sigmoid'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='rbf'</em>) – Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</p></li>
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>default=3</em>) – Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p></li>
<li><p><strong>gamma</strong> (<em>{'scale'</em><em>, </em><em>'auto'}</em><em> or </em><em>float</em><em>, </em><em>default='scale'</em>) – <p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<ul>
<li><p>if <code class="docutils literal notranslate"><span class="pre">gamma='scale'</span></code> (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,</p></li>
<li><p>if ‘auto’, uses 1 / n_features.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">gamma</span></code> changed from ‘auto’ to ‘scale’.</p>
</div>
</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Tolerance for stopping criterion.</p></li>
<li><p><strong>nu</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – An upper bound on the fraction of training
errors and a lower bound of the fraction of support
vectors. Should be in the interval (0, 1]. By default 0.5
will be taken.</p></li>
<li><p><strong>shrinking</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use the shrinking heuristic.
See the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>cache_size</strong> (<em>float</em><em>, </em><em>default=200</em>) – Specify the size of the kernel cache (in MB).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=-1</em>) – Hard limit on iterations within solver, or -1 for no limit.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.class_weight_">
<span class="sig-name descname"><span class="pre">class_weight_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.class_weight_" title="Permalink to this definition"></a></dt>
<dd><p>Multipliers of parameter C for each class.
Computed based on the <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.dual_coef_">
<span class="sig-name descname"><span class="pre">dual_coef_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.dual_coef_" title="Permalink to this definition"></a></dt>
<dd><p>Coefficients of the support vectors in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_SV)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.fit_status_">
<span class="sig-name descname"><span class="pre">fit_status_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.fit_status_" title="Permalink to this definition"></a></dt>
<dd><p>0 if correctly fitted, 1 otherwise (will raise warning)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constant in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.n_support_">
<span class="sig-name descname"><span class="pre">n_support_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.n_support_" title="Permalink to this definition"></a></dt>
<dd><p>Number of support vectors for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,), dtype=int32</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.offset_">
<span class="sig-name descname"><span class="pre">offset_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.offset_" title="Permalink to this definition"></a></dt>
<dd><p>Offset used to define the decision function from the raw scores.
We have the relation: decision_function = score_samples - <cite>offset_</cite>.
The offset is the opposite of <cite>intercept_</cite> and is provided for
consistency with other outlier detection algorithms.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.shape_fit_">
<span class="sig-name descname"><span class="pre">shape_fit_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.shape_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Array dimensions of training vector <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int of shape (n_dimensions_of_X,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.support_">
<span class="sig-name descname"><span class="pre">support_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.support_" title="Permalink to this definition"></a></dt>
<dd><p>Indices of support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.support_vectors_">
<span class="sig-name descname"><span class="pre">support_vectors_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.support_vectors_" title="Permalink to this definition"></a></dt>
<dd><p>Support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV, n_features)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">OneClassSVM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.44</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.45</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.46</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">OneClassSVM</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([-1,  1,  1,  1, -1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([1.7798..., 2.0547..., 2.0556..., 2.0561..., 1.7332...])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.OneClassSVM.fit" title="Permalink to this definition"></a></dt>
<dd><p>Detects the soft boundary of the set of samples X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Set of samples, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Per-sample weights. Rescale C per sample. Higher weights
force the classifier to put more emphasis on these points.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – not used, present for API consistency by convention.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If X is not a C-ordered contiguous array it is copied.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.OneClassSVM.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Signed distance to the separating hyperplane.</p>
<p>Signed distance is positive for an inlier and negative for an outlier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>dec</strong> – Returns the decision function of the samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.OneClassSVM.score_samples" title="Permalink to this definition"></a></dt>
<dd><p>Raw scoring function of the samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data matrix.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score_samples</strong> – Returns the (unshifted) scoring function of the samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.OneClassSVM.predict" title="Permalink to this definition"></a></dt>
<dd><p>Perform classification on samples in X.</p>
<p>For a one-class model, +1 or -1 is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples_test</em><em>, </em><em>n_samples_train</em><em>)</em>) – For kernel=”precomputed”, the expected shape of X is
(n_samples_test, n_samples_train).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_pred</strong> – Class labels for samples in X.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.probA_">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probA_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.probA_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.OneClassSVM.probB_">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probB_</span></span><a class="headerlink" href="#EIMTC.models.OneClassSVM.probB_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.SVC">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">SVC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'scale'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrinking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probability</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decision_function_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ovr'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">break_ties</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.SVC" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm._base.BaseSVC</span></code></p>
<p>C-Support Vector Classification.</p>
<p>The implementation is based on libsvm. The fit time scales at least
quadratically with the number of samples and may be impractical
beyond tens of thousands of samples. For large datasets
consider using <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">SGDClassifier</span></code> instead, possibly after a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Nystroem</span></code> transformer.</p>
<p>The multiclass support is handled according to a one-vs-one scheme.</p>
<p>For details on the precise mathematical formulation of the provided
kernel functions and how <cite>gamma</cite>, <cite>coef0</cite> and <cite>degree</cite> affect each
other, see the corresponding section in the narrative documentation:
<span class="xref std std-ref">svm_kernels</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Regularization parameter. The strength of the regularization is
inversely proportional to C. Must be strictly positive. The penalty
is a squared l2 penalty.</p></li>
<li><p><strong>kernel</strong> (<em>{'linear'</em><em>, </em><em>'poly'</em><em>, </em><em>'rbf'</em><em>, </em><em>'sigmoid'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='rbf'</em>) – Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to pre-compute the kernel matrix from data matrices; that matrix
should be an array of shape <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_samples)</span></code>.</p></li>
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>default=3</em>) – Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p></li>
<li><p><strong>gamma</strong> (<em>{'scale'</em><em>, </em><em>'auto'}</em><em> or </em><em>float</em><em>, </em><em>default='scale'</em>) – <p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<ul>
<li><p>if <code class="docutils literal notranslate"><span class="pre">gamma='scale'</span></code> (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,</p></li>
<li><p>if ‘auto’, uses 1 / n_features.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">gamma</span></code> changed from ‘auto’ to ‘scale’.</p>
</div>
</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p></li>
<li><p><strong>shrinking</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use the shrinking heuristic.
See the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>probability</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to enable probability estimates. This must be enabled prior
to calling <cite>fit</cite>, will slow down that method as it internally uses
5-fold cross-validation, and <cite>predict_proba</cite> may be inconsistent with
<cite>predict</cite>. Read more in the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Tolerance for stopping criterion.</p></li>
<li><p><strong>cache_size</strong> (<em>float</em><em>, </em><em>default=200</em>) – Specify the size of the kernel cache (in MB).</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em> or </em><em>'balanced'</em><em>, </em><em>default=None</em>) – Set the parameter C of class i to class_weight[i]*C for
SVC. If not given, all classes are supposed to have
weight one.
The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=-1</em>) – Hard limit on iterations within solver, or -1 for no limit.</p></li>
<li><p><strong>decision_function_shape</strong> (<em>{'ovo'</em><em>, </em><em>'ovr'}</em><em>, </em><em>default='ovr'</em>) – <p>Whether to return a one-vs-rest (‘ovr’) decision function of shape
(n_samples, n_classes) as all other classifiers, or the original
one-vs-one (‘ovo’) decision function of libsvm which has shape
(n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
(‘ovo’) is always used as multi-class strategy. The parameter is
ignored for binary classification.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>decision_function_shape is ‘ovr’ by default.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>decision_function_shape=’ovr’</em> is recommended.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=’ovo’ and None</em>.</p>
</div>
</p></li>
<li><p><strong>break_ties</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If true, <code class="docutils literal notranslate"><span class="pre">decision_function_shape='ovr'</span></code>, and number of classes &gt; 2,
<span class="xref std std-term">predict</span> will break ties according to the confidence values of
<span class="xref std std-term">decision_function</span>; otherwise the first class among the tied
classes is returned. Please note that breaking ties comes at a
relatively high computational cost compared to a simple predict.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the pseudo random number generation for shuffling the data for
probability estimates. Ignored when <cite>probability</cite> is False.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.class_weight_">
<span class="sig-name descname"><span class="pre">class_weight_</span></span><a class="headerlink" href="#EIMTC.models.SVC.class_weight_" title="Permalink to this definition"></a></dt>
<dd><p>Multipliers of parameter C for each class.
Computed based on the <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.SVC.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.SVC.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p><cite>coef_</cite> is a readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.dual_coef_">
<span class="sig-name descname"><span class="pre">dual_coef_</span></span><a class="headerlink" href="#EIMTC.models.SVC.dual_coef_" title="Permalink to this definition"></a></dt>
<dd><p>Dual coefficients of the support vector in the decision
function (see <span class="xref std std-ref">sgd_mathematical_formulation</span>), multiplied by
their targets.
For multiclass, coefficient for all 1-vs-1 classifiers.
The layout of the coefficients in the multiclass case is somewhat
non-trivial. See the <span class="xref std std-ref">multi-class section of the User Guide</span> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes -1, n_SV)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.fit_status_">
<span class="sig-name descname"><span class="pre">fit_status_</span></span><a class="headerlink" href="#EIMTC.models.SVC.fit_status_" title="Permalink to this definition"></a></dt>
<dd><p>0 if correctly fitted, 1 otherwise (will raise warning)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.SVC.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.support_">
<span class="sig-name descname"><span class="pre">support_</span></span><a class="headerlink" href="#EIMTC.models.SVC.support_" title="Permalink to this definition"></a></dt>
<dd><p>Indices of support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.support_vectors_">
<span class="sig-name descname"><span class="pre">support_vectors_</span></span><a class="headerlink" href="#EIMTC.models.SVC.support_vectors_" title="Permalink to this definition"></a></dt>
<dd><p>Support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.n_support_">
<span class="sig-name descname"><span class="pre">n_support_</span></span><a class="headerlink" href="#EIMTC.models.SVC.n_support_" title="Permalink to this definition"></a></dt>
<dd><p>Number of support vectors for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,), dtype=int32</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.probA_">
<span class="sig-name descname"><span class="pre">probA_</span></span><a class="headerlink" href="#EIMTC.models.SVC.probA_" title="Permalink to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.probB_">
<span class="sig-name descname"><span class="pre">probB_</span></span><a class="headerlink" href="#EIMTC.models.SVC.probB_" title="Permalink to this definition"></a></dt>
<dd><p>If <cite>probability=True</cite>, it corresponds to the parameters learned in
Platt scaling to produce probability estimates from decision values.
If <cite>probability=False</cite>, it’s an empty array. Platt scaling uses the
logistic function
<code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(decision_value</span> <span class="pre">*</span> <span class="pre">probA_</span> <span class="pre">+</span> <span class="pre">probB_))</span></code>
where <code class="docutils literal notranslate"><span class="pre">probA_</span></code> and <code class="docutils literal notranslate"><span class="pre">probB_</span></code> are learned from the dataset <a href="#id74"><span class="problematic" id="id17">[2]_</span></a>. For
more information on the multiclass case and training procedure see
section 8 of <a href="#id75"><span class="problematic" id="id18">[1]_</span></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes * (n_classes - 1) / 2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVC.shape_fit_">
<span class="sig-name descname"><span class="pre">shape_fit_</span></span><a class="headerlink" href="#EIMTC.models.SVC.shape_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Array dimensions of training vector <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int of shape (n_dimensions_of_X,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;svc&#39;, SVC(gamma=&#39;auto&#39;))])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.SVR" title="EIMTC.models.SVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SVR</span></code></a></dt><dd><p>Support Vector Machine for Regression implemented using libsvm.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.LinearSVC" title="EIMTC.models.LinearSVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearSVC</span></code></a></dt><dd><p>Scalable Linear Support Vector Machine for classification implemented using liblinear. Check the See Also section of LinearSVC for more comparison element.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id19"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
</dd>
<dt class="label" id="id21"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639">Platt, John (1999). “Probabilistic outputs for support vector
machines and comparison to regularizedlikelihood methods.”</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.SVR">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">SVR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'rbf'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'scale'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrinking</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-</span> <span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.SVR" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm._base.BaseLibSVM</span></code></p>
<p>Epsilon-Support Vector Regression.</p>
<p>The free parameters in the model are C and epsilon.</p>
<p>The implementation is based on libsvm. The fit time complexity
is more than quadratic with the number of samples which makes it hard
to scale to datasets with more than a couple of 10000 samples. For large
datasets consider using <code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVR</span></code> or
<code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code> instead, possibly after a
<code class="xref py py-class docutils literal notranslate"><span class="pre">Nystroem</span></code> transformer.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>kernel</strong> (<em>{'linear'</em><em>, </em><em>'poly'</em><em>, </em><em>'rbf'</em><em>, </em><em>'sigmoid'</em><em>, </em><em>'precomputed'}</em><em>, </em><em>default='rbf'</em>) – Specifies the kernel type to be used in the algorithm.
It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
a callable.
If none is given, ‘rbf’ will be used. If a callable is given it is
used to precompute the kernel matrix.</p></li>
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>default=3</em>) – Degree of the polynomial kernel function (‘poly’).
Ignored by all other kernels.</p></li>
<li><p><strong>gamma</strong> (<em>{'scale'</em><em>, </em><em>'auto'}</em><em> or </em><em>float</em><em>, </em><em>default='scale'</em>) – <p>Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
<ul>
<li><p>if <code class="docutils literal notranslate"><span class="pre">gamma='scale'</span></code> (default) is passed then it uses
1 / (n_features * X.var()) as value of gamma,</p></li>
<li><p>if ‘auto’, uses 1 / n_features.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">gamma</span></code> changed from ‘auto’ to ‘scale’.</p>
</div>
</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – Independent term in kernel function.
It is only significant in ‘poly’ and ‘sigmoid’.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Tolerance for stopping criterion.</p></li>
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Regularization parameter. The strength of the regularization is
inversely proportional to C. Must be strictly positive.
The penalty is a squared l2 penalty.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Epsilon in the epsilon-SVR model. It specifies the epsilon-tube
within which no penalty is associated in the training loss function
with points predicted within a distance epsilon from the actual
value.</p></li>
<li><p><strong>shrinking</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use the shrinking heuristic.
See the <span class="xref std std-ref">User Guide</span>.</p></li>
<li><p><strong>cache_size</strong> (<em>float</em><em>, </em><em>default=200</em>) – Specify the size of the kernel cache (in MB).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Enable verbose output. Note that this setting takes advantage of a
per-process runtime setting in libsvm that, if enabled, may not work
properly in a multithreaded context.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=-1</em>) – Hard limit on iterations within solver, or -1 for no limit.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.class_weight_">
<span class="sig-name descname"><span class="pre">class_weight_</span></span><a class="headerlink" href="#EIMTC.models.SVR.class_weight_" title="Permalink to this definition"></a></dt>
<dd><p>Multipliers of parameter C for each class.
Computed based on the <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.SVR.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Weights assigned to the features (coefficients in the primal
problem). This is only available in the case of a linear kernel.</p>
<p><cite>coef_</cite> is readonly property derived from <cite>dual_coef_</cite> and
<cite>support_vectors_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.dual_coef_">
<span class="sig-name descname"><span class="pre">dual_coef_</span></span><a class="headerlink" href="#EIMTC.models.SVR.dual_coef_" title="Permalink to this definition"></a></dt>
<dd><p>Coefficients of the support vector in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_SV)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.fit_status_">
<span class="sig-name descname"><span class="pre">fit_status_</span></span><a class="headerlink" href="#EIMTC.models.SVR.fit_status_" title="Permalink to this definition"></a></dt>
<dd><p>0 if correctly fitted, 1 otherwise (will raise warning)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.SVR.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.n_support_">
<span class="sig-name descname"><span class="pre">n_support_</span></span><a class="headerlink" href="#EIMTC.models.SVR.n_support_" title="Permalink to this definition"></a></dt>
<dd><p>Number of support vectors for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,), dtype=int32</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.shape_fit_">
<span class="sig-name descname"><span class="pre">shape_fit_</span></span><a class="headerlink" href="#EIMTC.models.SVR.shape_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Array dimensions of training vector <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>tuple of int of shape (n_dimensions_of_X,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.support_">
<span class="sig-name descname"><span class="pre">support_</span></span><a class="headerlink" href="#EIMTC.models.SVR.support_" title="Permalink to this definition"></a></dt>
<dd><p>Indices of support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.SVR.support_vectors_">
<span class="sig-name descname"><span class="pre">support_vectors_</span></span><a class="headerlink" href="#EIMTC.models.SVR.support_vectors_" title="Permalink to this definition"></a></dt>
<dd><p>Support vectors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_SV, n_features)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">SVR</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;svr&#39;, SVR(epsilon=0.2))])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.NuSVR" title="EIMTC.models.NuSVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NuSVR</span></code></a></dt><dd><p>Support Vector Machine for regression implemented using libsvm using a parameter to control the number of support vectors.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.LinearSVR" title="EIMTC.models.LinearSVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LinearSVR</span></code></a></dt><dd><p>Scalable Linear Support Vector Machine for regression implemented using liblinear.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id23"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: A Library for Support Vector Machines</a></p>
</dd>
<dt class="label" id="id25"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639">Platt, John (1999). “Probabilistic outputs for support vector
machines and comparison to regularizedlikelihood methods.”</a></p>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.SVR.probA_">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probA_</span></span><a class="headerlink" href="#EIMTC.models.SVR.probA_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.SVR.probB_">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">probB_</span></span><a class="headerlink" href="#EIMTC.models.SVR.probB_" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">ExtraTreesClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestClassifier</span></code></p>
<p>An extra-trees classifier.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Controls 3 sources of randomness:</p>
<ul>
<li><p>the bootstrapping of the samples used when building trees
(if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>)</p></li>
<li><p>the sampling of the features to consider when looking for the best
split at each node (if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>)</p></li>
<li><p>the draw of the splits for each of the <cite>max_features</cite></p></li>
</ul>
<p>See <span class="xref std std-term">Glossary</span> for details.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>class_weight</strong> (<em>{&quot;balanced&quot;</em><em>, </em><em>&quot;balanced_subsample&quot;}</em><em>, </em><em>dict</em><em> or </em><em>list of dicts</em><em>,             </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The “balanced_subsample” mode is the same as “balanced” except that
weights are computed based on the bootstrap sample for every tree
grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.ExtraTreesClassifier" title="EIMTC.models.ExtraTreesClassifier">ExtraTreesClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or a list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.oob_score_" title="Permalink to this definition"></a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesClassifier.oob_decision_function_">
<span class="sig-name descname"><span class="pre">oob_decision_function_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesClassifier.oob_decision_function_" title="Permalink to this definition"></a></dt>
<dd><p>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN. This attribute exists
only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.ExtraTreeClassifier</span></code></dt><dd><p>Base classifier for this ensemble.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.RandomForestClassifier" title="EIMTC.models.RandomForestClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a></dt><dd><p>Ensemble Classifier based on trees with optimal splits.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id27"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ExtraTreesClassifier(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">AdaBoostClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SAMME.R'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._weight_boosting.BaseWeightBoosting</span></code></p>
<p>An AdaBoost classifier.</p>
<p>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost-SAMME [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.14.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator from which the boosted ensemble is built.
Support for sample weighting is required, as well as proper
<code class="docutils literal notranslate"><span class="pre">classes_</span></code> and <code class="docutils literal notranslate"><span class="pre">n_classes_</span></code> attributes. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then
the base estimator is <code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>
initialized with <cite>max_depth=1</cite>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=50</em>) – The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=1.</em>) – Weight applied to each classifier at each boosting iteration. A higher
learning rate increases the contribution of each classifier. There is
a trade-off between the <cite>learning_rate</cite> and <cite>n_estimators</cite> parameters.</p></li>
<li><p><strong>algorithm</strong> (<em>{'SAMME'</em><em>, </em><em>'SAMME.R'}</em><em>, </em><em>default='SAMME.R'</em>) – If ‘SAMME.R’ then use the SAMME.R real boosting algorithm.
<code class="docutils literal notranslate"><span class="pre">base_estimator</span></code> must support calculation of class probabilities.
If ‘SAMME’ then use the SAMME discrete boosting algorithm.
The SAMME.R algorithm typically converges faster than SAMME,
achieving a lower test error with fewer boosting iterations.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given at each <cite>base_estimator</cite> at each
boosting iteration.
Thus, it is only used when <cite>base_estimator</cite> exposes a <cite>random_state</cite>.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of classifiers</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.estimator_weights_">
<span class="sig-name descname"><span class="pre">estimator_weights_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.estimator_weights_" title="Permalink to this definition"></a></dt>
<dd><p>Weights for each estimator in the boosted ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of floats</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.estimator_errors_">
<span class="sig-name descname"><span class="pre">estimator_errors_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.estimator_errors_" title="Permalink to this definition"></a></dt>
<dd><p>Classification error for each estimator in the boosted
ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of floats</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances if supported by the
<code class="docutils literal notranslate"><span class="pre">base_estimator</span></code> (when based on decision trees).</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">AdaBoostRegressor</span></code></dt><dd><p>An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.GradientBoostingClassifier" title="EIMTC.models.GradientBoostingClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a></dt><dd><p>GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeClassifier</span></code></dt><dd><p>A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id28"><span class="brackets">1</span></dt>
<dd><p>Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.</p>
</dd>
<dt class="label" id="id29"><span class="brackets">2</span></dt>
<dd><ol class="upperalpha simple" start="10">
<li><p>Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.</p></li>
</ol>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">AdaBoostClassifier(n_estimators=100, random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.983...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.fit" title="Permalink to this definition"></a></dt>
<dd><p>Build a boosted classifier from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The target values (class labels).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, the sample weights are initialized to
<code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">n_samples</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict classes for X.</p>
<p>The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.staged_predict" title="Permalink to this definition"></a></dt>
<dd><p>Return staged predictions for X.</p>
<p>The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.</p>
<p>This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>y</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The predicted classes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Compute the decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.
Binary classification is a special cases with <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>,
otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>. For binary classification,
values closer to -1 or 1 mean more like the first or second
class in <code class="docutils literal notranslate"><span class="pre">classes_</span></code>, respectively.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape of (n_samples, k)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.staged_decision_function">
<span class="sig-name descname"><span class="pre">staged_decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.staged_decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Compute decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code> for each boosting iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each boosting iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>score</strong> (<em>generator of ndarray of shape (n_samples, k)</em>) – The decision function of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.
Binary classification is a special cases with <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>,
otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>. For binary classification,
values closer to -1 or 1 mean more like the first or second
class in <code class="docutils literal notranslate"><span class="pre">classes_</span></code>, respectively.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.staged_predict_proba">
<span class="sig-name descname"><span class="pre">staged_predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.staged_predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.</p>
<p>This generator method yields the ensemble predicted class probabilities
after each iteration of boosting and therefore allows monitoring, such
as to determine the predicted class probabilities on a test set after
each boost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>p</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The class probabilities of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.AdaBoostClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.AdaBoostClassifier.predict_log_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class log-probabilities for X.</p>
<p>The predicted class log-probabilities of an input sample is computed as
the weighted mean predicted class log-probabilities of the classifiers
in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">BaggingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._bagging.BaseBagging</span></code></p>
<p>A Bagging regressor.</p>
<p>A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id76"><span class="problematic" id="id30">[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id77"><span class="problematic" id="id31">[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id78"><span class="problematic" id="id32">[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id79"><span class="problematic" id="id33">[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a
<code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=10</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of samples to draw from X to train each base estimator (with
replacement by default, see <cite>bootstrap</cite> for more details).</p>
<ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
</ul>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of features to draw from X to train each base estimator (
without replacement by default, see <cite>bootstrap_features</cite> for more
details).</p>
<ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether samples are drawn with replacement. If False, sampling
without replacement is performed.</p></li>
<li><p><strong>bootstrap_features</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether features are drawn with replacement.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate
the generalization error. Only available if bootstrap=True.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for both <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and
<a class="reference internal" href="#EIMTC.models.BaggingRegressor.predict" title="EIMTC.models.BaggingRegressor.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all
processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random resampling of the original dataset
(sample wise and feature wise).
If the base estimator accepts a <cite>random_state</cite> attribute, a different
seed is generated for each instance in the ensemble.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.estimators_samples_">
<span class="sig-name descname"><span class="pre">estimators_samples_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.estimators_samples_" title="Permalink to this definition"></a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.estimators_features_">
<span class="sig-name descname"><span class="pre">estimators_features_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.estimators_features_" title="Permalink to this definition"></a></dt>
<dd><p>The subset of drawn features for each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.oob_score_" title="Permalink to this definition"></a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.oob_prediction_">
<span class="sig-name descname"><span class="pre">oob_prediction_</span></span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.oob_prediction_" title="Permalink to this definition"></a></dt>
<dd><p>Prediction computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_prediction_</cite> might contain NaN. This attribute exists only
when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_targets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">SVR</span><span class="p">(),</span>
<span class="gp">... </span>                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([-2.8720...])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id34"><span class="brackets">1</span></dt>
<dd><p>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140,
1996.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">3</span></dt>
<dd><p>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</p>
</dd>
<dt class="label" id="id37"><span class="brackets">4</span></dt>
<dd><p>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.BaggingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingRegressor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict regression target for X.</p>
<p>The predicted regression target of an input sample is computed as the
mean predicted regression targets of the estimators in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">BaggingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._bagging.BaseBagging</span></code></p>
<p>A Bagging classifier.</p>
<p>A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id80"><span class="problematic" id="id38">[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id81"><span class="problematic" id="id39">[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id82"><span class="problematic" id="id40">[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id83"><span class="problematic" id="id41">[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a
<code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=10</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of samples to draw from X to train each base estimator (with
replacement by default, see <cite>bootstrap</cite> for more details).</p>
<ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
</ul>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of features to draw from X to train each base estimator (
without replacement by default, see <cite>bootstrap_features</cite> for more
details).</p>
<ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether samples are drawn with replacement. If False, sampling
without replacement is performed.</p></li>
<li><p><strong>bootstrap_features</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether features are drawn with replacement.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate
the generalization error. Only available if bootstrap=True.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>warm_start</em> constructor parameter.</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for both <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and
<a class="reference internal" href="#EIMTC.models.BaggingClassifier.predict" title="EIMTC.models.BaggingClassifier.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all
processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random resampling of the original dataset
(sample wise and feature wise).
If the base estimator accepts a <cite>random_state</cite> attribute, a different
seed is generated for each instance in the ensemble.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted base estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.estimators_samples_">
<span class="sig-name descname"><span class="pre">estimators_samples_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.estimators_samples_" title="Permalink to this definition"></a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.estimators_features_">
<span class="sig-name descname"><span class="pre">estimators_features_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.estimators_features_" title="Permalink to this definition"></a></dt>
<dd><p>The subset of drawn features for each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.oob_score_" title="Permalink to this definition"></a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.oob_decision_function_">
<span class="sig-name descname"><span class="pre">oob_decision_function_</span></span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.oob_decision_function_" title="Permalink to this definition"></a></dt>
<dd><p>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN. This attribute exists
only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">SVC</span><span class="p">(),</span>
<span class="gp">... </span>                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id42"><span class="brackets">1</span></dt>
<dd><p>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</p>
</dd>
<dt class="label" id="id43"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140,
1996.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">3</span></dt>
<dd><p>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</p>
</dd>
<dt class="label" id="id45"><span class="brackets">4</span></dt>
<dd><p>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict class for X.</p>
<p>The predicted class of an input sample is computed as the class with
the highest mean predicted probability. If base estimators do not
implement a <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method, then it resorts to voting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the mean predicted class probabilities of the base estimators in the
ensemble. If base estimators do not implement a <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>
method, then it resorts to voting and the predicted class probabilities
of an input sample represents the proportion of estimators predicting
each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.predict_log_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class log-probabilities for X.</p>
<p>The predicted class log-probabilities of an input sample is computed as
the log of the mean predicted class probabilities of the base
estimators in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.BaggingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BaggingClassifier.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Average of the decision functions of the base classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples. The columns correspond
to the classes in sorted order, as they appear in the attribute
<code class="docutils literal notranslate"><span class="pre">classes_</span></code>. Regression and binary classification are special
cases with <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, k)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">ExtraTreesRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestRegressor</span></code></p>
<p>An extra-trees regressor.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;mse&quot;</em><em>, </em><em>&quot;mae&quot;}</em><em>, </em><em>default=&quot;mse&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion, and “mae” for the mean
absolute error.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Controls 3 sources of randomness:</p>
<ul>
<li><p>the bootstrapping of the samples used when building trees
(if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>)</p></li>
<li><p>the sampling of the features to consider when looking for the best
split at each node (if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>)</p></li>
<li><p>the draw of the splits for each of the <cite>max_features</cite></p></li>
</ul>
<p>See <span class="xref std std-term">Glossary</span> for details.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.ExtraTreeRegressor" title="EIMTC.models.ExtraTreeRegressor">ExtraTreeRegressor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeRegressor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.oob_score_" title="Permalink to this definition"></a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreesRegressor.oob_prediction_">
<span class="sig-name descname"><span class="pre">oob_prediction_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreesRegressor.oob_prediction_" title="Permalink to this definition"></a></dt>
<dd><p>Prediction computed with out-of-bag estimate on the training set.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.ExtraTreeRegressor</span></code></dt><dd><p>Base estimator for this ensemble.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.RandomForestRegressor" title="EIMTC.models.RandomForestRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code></a></dt><dd><p>Ensemble regressor using trees with optimal splits.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id46"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">ExtraTreesRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.2708...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">GradientBoostingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deviance'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'friedman_mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._gb.BaseGradientBoosting</span></code></p>
<p>Gradient Boosting for classification.</p>
<p>GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <code class="docutils literal notranslate"><span class="pre">n_classes_</span></code>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>{'deviance'</em><em>, </em><em>'exponential'}</em><em>, </em><em>default='deviance'</em>) – The loss function to be optimized. ‘deviance’ refers to
deviance (= logistic regression) for classification
with probabilistic outputs. For loss ‘exponential’ gradient
boosting recovers the AdaBoost algorithm.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</p></li>
<li><p><strong>subsample</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</p></li>
<li><p><strong>criterion</strong> (<em>{'friedman_mse'</em><em>, </em><em>'mse'</em><em>, </em><em>'mae'}</em><em>, </em><em>default='friedman_mse'</em>) – <p>The function to measure the quality of a split. Supported criteria
are ‘friedman_mse’ for the mean squared error with improvement
score by Friedman, ‘mse’ for mean squared error, and ‘mae’ for
the mean absolute error. The default value of ‘friedman_mse’ is
generally the best as it can provide a better approximation in
some cases.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><cite>criterion=’mae’</cite> is deprecated and will be removed in version
1.1 (renaming of 0.26). Use <cite>criterion=’friedman_mse’</cite> or <cite>‘mse’</cite>
instead, as trees should use a least-square criterion in
Gradient Boosting.</p>
</div>
</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=3</em>) – The maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>init</strong> (<em>estimator</em><em> or </em><em>'zero'</em><em>, </em><em>default=None</em>) – An estimator object that is used to compute the initial predictions.
<code class="docutils literal notranslate"><span class="pre">init</span></code> has to provide <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and <a class="reference internal" href="#EIMTC.models.GradientBoostingClassifier.predict_proba" title="EIMTC.models.GradientBoostingClassifier.predict_proba"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_proba()</span></code></a>. If
‘zero’, the initial raw predictions are set to zero. By default, a
<code class="docutils literal notranslate"><span class="pre">DummyEstimator</span></code> predicting the classes priors is used.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random spliting of the training data to obtain a
validation set if <cite>n_iter_no_change</cite> is not None.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>max_features</strong> (<em>{'auto'</em><em>, </em><em>'sqrt'</em><em>, </em><em>'log2'}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If ‘auto’, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If ‘sqrt’, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If ‘log2’, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is set to an integer.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> size of the training
data as validation and terminate training when validation score is not
improving in all of the previous <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> numbers of
iterations. The split is stratified.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – <p>Tolerance for the early stopping. When the loss is not improving
by at least tol for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> iterations (if set to a
number), the training stops.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.n_estimators_">
<span class="sig-name descname"><span class="pre">n_estimators_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.n_estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The number of estimators as selected by early stopping (if
<code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is specified). Otherwise it is set to
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.oob_improvement_">
<span class="sig-name descname"><span class="pre">oob_improvement_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.oob_improvement_" title="Permalink to this definition"></a></dt>
<dd><p>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal notranslate"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal notranslate"><span class="pre">init</span></code> estimator.
Only available if <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">&lt;</span> <span class="pre">1.0</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.train_score_">
<span class="sig-name descname"><span class="pre">train_score_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.train_score_" title="Permalink to this definition"></a></dt>
<dd><p>The i-th score <code class="docutils literal notranslate"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal notranslate"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.loss_" title="Permalink to this definition"></a></dt>
<dd><p>The concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>LossFunction</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.init_">
<span class="sig-name descname"><span class="pre">init_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.init_" title="Permalink to this definition"></a></dt>
<dd><p>The estimator that provides the initial predictions.
Set via the <code class="docutils literal notranslate"><span class="pre">init</span></code> argument or <code class="docutils literal notranslate"><span class="pre">loss.init_estimator</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators. <code class="docutils literal notranslate"><span class="pre">loss_.K</span></code> is 1 for binary
classification, otherwise n_classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of DecisionTreeRegressor of shape (n_estimators, <code class="docutils literal notranslate"><span class="pre">loss_.K</span></code>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of data features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.max_features_" title="Permalink to this definition"></a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></dt><dd><p>Histogram-based Gradient Boosting Classification Tree.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeClassifier</span></code></dt><dd><p>A decision tree classifier.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.RandomForestClassifier" title="EIMTC.models.RandomForestClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a></dt><dd><p>A meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</p>
</dd>
<dt><a class="reference internal" href="#EIMTC.models.AdaBoostClassifier" title="EIMTC.models.AdaBoostClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a></dt><dd><p>A meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p class="rubric">References</p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li><p>Friedman, Stochastic Gradient Boosting, 1999</p></li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<p class="rubric">Examples</p>
<p>The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.913...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Compute the decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
order of the classes corresponds to that in the attribute
<span class="xref std std-term">classes_</span>. Regression and binary classification produce an
array of shape (n_samples,).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.staged_decision_function">
<span class="sig-name descname"><span class="pre">staged_decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.staged_decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Compute decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code> for each iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.
Regression and binary classification are special cases with
<code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples, k)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict class for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.staged_predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict class at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted value of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the <code class="docutils literal notranslate"><span class="pre">loss</span></code> does not support probabilities.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>p</strong> – The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.predict_log_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class log-probabilities for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the <code class="docutils literal notranslate"><span class="pre">loss</span></code> does not support probabilities.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>p</strong> – The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingClassifier.staged_predict_proba">
<span class="sig-name descname"><span class="pre">staged_predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingClassifier.staged_predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted value of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">GradientBoostingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ls'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'friedman_mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._gb.BaseGradientBoosting</span></code></p>
<p>Gradient Boosting for regression.</p>
<p>GB builds an additive model in a forward stage-wise fashion;
it allows for the optimization of arbitrary differentiable loss functions.
In each stage a regression tree is fit on the negative gradient of the
given loss function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>{'ls'</em><em>, </em><em>'lad'</em><em>, </em><em>'huber'</em><em>, </em><em>'quantile'}</em><em>, </em><em>default='ls'</em>) – Loss function to be optimized. ‘ls’ refers to least squares
regression. ‘lad’ (least absolute deviation) is a highly robust
loss function solely based on order information of the input
variables. ‘huber’ is a combination of the two. ‘quantile’
allows quantile regression (use <cite>alpha</cite> to specify the quantile).</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</p></li>
<li><p><strong>subsample</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</p></li>
<li><p><strong>criterion</strong> (<em>{'friedman_mse'</em><em>, </em><em>'mse'</em><em>, </em><em>'mae'}</em><em>, </em><em>default='friedman_mse'</em>) – <p>The function to measure the quality of a split. Supported criteria
are “friedman_mse” for the mean squared error with improvement
score by Friedman, “mse” for mean squared error, and “mae” for
the mean absolute error. The default value of “friedman_mse” is
generally the best as it can provide a better approximation in
some cases.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><cite>criterion=’mae’</cite> is deprecated and will be removed in version
1.1 (renaming of 0.26). The correct way of minimizing the absolute
error is to use <cite>loss=’lad’</cite> instead.</p>
</div>
</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=3</em>) – Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>init</strong> (<em>estimator</em><em> or </em><em>'zero'</em><em>, </em><em>default=None</em>) – An estimator object that is used to compute the initial predictions.
<code class="docutils literal notranslate"><span class="pre">init</span></code> has to provide <span class="xref std std-term">fit</span> and <span class="xref std std-term">predict</span>. If ‘zero’, the
initial raw predictions are set to zero. By default a
<code class="docutils literal notranslate"><span class="pre">DummyEstimator</span></code> is used, predicting either the average target value
(for loss=’ls’), or a quantile for the other losses.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random spliting of the training data to obtain a
validation set if <cite>n_iter_no_change</cite> is not None.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>max_features</strong> (<em>{'auto'</em><em>, </em><em>'sqrt'</em><em>, </em><em>'log2'}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.9</em>) – The alpha-quantile of the huber loss function and the quantile
loss function. Only if <code class="docutils literal notranslate"><span class="pre">loss='huber'</span></code> or <code class="docutils literal notranslate"><span class="pre">loss='quantile'</span></code>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is set to an integer.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> size of the training
data as validation and terminate training when validation score is not
improving in all of the previous <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> numbers of
iterations.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – <p>Tolerance for the early stopping. When the loss is not improving
by at least tol for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> iterations (if set to a
number), the training stops.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.oob_improvement_">
<span class="sig-name descname"><span class="pre">oob_improvement_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.oob_improvement_" title="Permalink to this definition"></a></dt>
<dd><p>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal notranslate"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal notranslate"><span class="pre">init</span></code> estimator.
Only available if <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">&lt;</span> <span class="pre">1.0</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.train_score_">
<span class="sig-name descname"><span class="pre">train_score_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.train_score_" title="Permalink to this definition"></a></dt>
<dd><p>The i-th score <code class="docutils literal notranslate"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal notranslate"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.loss_" title="Permalink to this definition"></a></dt>
<dd><p>The concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>LossFunction</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.init_">
<span class="sig-name descname"><span class="pre">init_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.init_" title="Permalink to this definition"></a></dt>
<dd><p>The estimator that provides the initial predictions.
Set via the <code class="docutils literal notranslate"><span class="pre">init</span></code> argument or <code class="docutils literal notranslate"><span class="pre">loss.init_estimator</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of DecisionTreeRegressor of shape (n_estimators, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes, set to 1 for regressors.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span>Attribute <code class="docutils literal notranslate"><span class="pre">n_classes_</span></code> was deprecated in version 0.24 and
will be removed in 1.1 (renaming of 0.26).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.n_estimators_">
<span class="sig-name descname"><span class="pre">n_estimators_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.n_estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The number of estimators as selected by early stopping (if
<code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is specified). Otherwise it is set to
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of data features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.max_features_" title="Permalink to this definition"></a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></dt><dd><p>Histogram-based Gradient Boosting Classification Tree.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code></dt><dd><p>A decision tree regressor.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.RandomForestRegressor</span></code></dt><dd><p>A random forest regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">GradientBoostingRegressor(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="go">array([-61...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.4...</span>
</pre></div>
</div>
<p class="rubric">References</p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li><p>Friedman, Stochastic Gradient Boosting, 1999</p></li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict regression target for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.staged_predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict regression target at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted value of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GradientBoostingRegressor.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GradientBoostingRegressor.apply" title="Permalink to this definition"></a></dt>
<dd><p>Apply trees in the ensemble to X, return leaf indices.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, its dtype will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code>. If a sparse matrix is provided, it will
be converted to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples, n_estimators)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id47">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#id47" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">IsolationForest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.IsolationForest" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.OutlierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._bagging.BaseBagging</span></code></p>
<p>Isolation Forest Algorithm.</p>
<p>Return the anomaly score of each sample using the IsolationForest algorithm</p>
<p>The IsolationForest ‘isolates’ observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.</p>
<p>Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.</p>
<p>This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.</p>
<p>Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>&quot;auto&quot;</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <dl class="simple">
<dt>The number of samples to draw from X to train each base estimator.</dt><dd><ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
<li><p>If “auto”, then <cite>max_samples=min(256, n_samples)</cite>.</p></li>
</ul>
</dd>
</dl>
<p>If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).</p>
</p></li>
<li><p><strong>contamination</strong> (<em>'auto'</em><em> or </em><em>float</em><em>, </em><em>default='auto'</em>) – <p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the scores of the samples.</p>
<blockquote>
<div><ul>
<li><p>If ‘auto’, the threshold is determined as in the
original paper.</p></li>
<li><p>If float, the contamination should be in the range [0, 0.5].</p></li>
</ul>
</div></blockquote>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">contamination</span></code> changed from 0.1
to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>.</p>
</div>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of features to draw from X to train each base estimator.</p>
<blockquote>
<div><ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for both <a class="reference internal" href="#EIMTC.models.IsolationForest.fit" title="EIMTC.models.IsolationForest.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> and
<a class="reference internal" href="#EIMTC.models.IsolationForest.predict" title="EIMTC.models.IsolationForest.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all
processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Controls the pseudo-randomness of the selection of the feature
and split values for each branching step and each tree in the forest.</p>
<p>Pass an int for reproducible results across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity of the tree building process.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The child estimator template used to create the collection of
fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ExtraTreeRegressor instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ExtraTreeRegressor instances</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.estimators_features_">
<span class="sig-name descname"><span class="pre">estimators_features_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.estimators_features_" title="Permalink to this definition"></a></dt>
<dd><p>The subset of drawn features for each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.estimators_samples_">
<span class="sig-name descname"><span class="pre">estimators_samples_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.estimators_samples_" title="Permalink to this definition"></a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.max_samples_">
<span class="sig-name descname"><span class="pre">max_samples_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.max_samples_" title="Permalink to this definition"></a></dt>
<dd><p>The actual number of samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.offset_">
<span class="sig-name descname"><span class="pre">offset_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.offset_" title="Permalink to this definition"></a></dt>
<dd><p>Offset used to define the decision function from the raw scores. We
have the relation: <code class="docutils literal notranslate"><span class="pre">decision_function</span> <span class="pre">=</span> <span class="pre">score_samples</span> <span class="pre">-</span> <span class="pre">offset_</span></code>.
<code class="docutils literal notranslate"><span class="pre">offset_</span></code> is defined as follows. When the contamination parameter is
set to “auto”, the offset is equal to -0.5 as the scores of inliers are
close to 0 and the scores of outliers are close to -1. When a
contamination parameter different than “auto” is provided, the offset
is defined in such a way we obtain the expected number of outliers
(samples with decision function &lt; 0) in training.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.IsolationForest.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to <code class="docutils literal notranslate"><span class="pre">ceil(log_2(n))</span></code> where
<span class="math notranslate nohighlight">\(n\)</span> is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id48"><span class="brackets">1</span></dt>
<dd><p>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.</p>
</dd>
<dt class="label" id="id49"><span class="brackets">2</span></dt>
<dd><p>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based
anomaly detection.” ACM Transactions on Knowledge Discovery from
Data (TKDD) 6.1 (2012): 3.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.covariance.EllipticEnvelope</span></code></dt><dd><p>An object for detecting outliers in a Gaussian distributed dataset.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.OneClassSVM</span></code></dt><dd><p>Unsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.neighbors.LocalOutlierFactor</span></code></dt><dd><p>Unsupervised Outlier Detection using Local Outlier Factor (LOF).</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">IsolationForest</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">90</span><span class="p">]])</span>
<span class="go">array([ 1,  1, -1])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.IsolationForest.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for maximum
efficiency. Sparse matrices are also supported, use sparse
<code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code> for maximum efficiency.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.IsolationForest.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>is_inlier</strong> – For each observation, tells whether or not (+1 or -1) it should
be considered as an inlier according to the fitted model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.IsolationForest.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Average anomaly score of X of the base classifiers.</p>
<p>The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.</p>
<p>The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scores</strong> – The anomaly score of the input samples.
The lower, the more abnormal. Negative scores represent outliers,
positive scores represent inliers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.IsolationForest.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.IsolationForest.score_samples" title="Permalink to this definition"></a></dt>
<dd><p>Opposite of the anomaly score defined in the original paper.</p>
<p>The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.</p>
<p>The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scores</strong> – The anomaly score of the input samples.
The lower, the more abnormal.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">RandomForestClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestClassifier</span></code></p>
<p>A random forest classifier.</p>
<p>A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <cite>max_samples</cite> parameter if
<cite>bootstrap=True</cite> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.
Note: this parameter is tree-specific.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite> (same as “auto”).</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls both the randomness of the bootstrapping of the samples used
when building trees (if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>) and the sampling of the
features to consider when looking for the best split at each node
(if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>).
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>class_weight</strong> (<em>{&quot;balanced&quot;</em><em>, </em><em>&quot;balanced_subsample&quot;}</em><em>, </em><em>dict</em><em> or </em><em>list of dicts</em><em>,             </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The “balanced_subsample” mode is the same as “balanced” except that
weights are computed based on the bootstrap sample for every tree
grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.DecisionTreeClassifier" title="EIMTC.models.DecisionTreeClassifier">DecisionTreeClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or a list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.oob_score_" title="Permalink to this definition"></a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestClassifier.oob_decision_function_">
<span class="sig-name descname"><span class="pre">oob_decision_function_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestClassifier.oob_decision_function_" title="Permalink to this definition"></a></dt>
<dd><p>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN. This attribute exists
only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.DecisionTreeClassifier" title="EIMTC.models.DecisionTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.ExtraTreesClassifier" title="EIMTC.models.ExtraTreesClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreesClassifier</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id50"><span class="brackets">1</span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</p></li>
</ol>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">RandomForestRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestRegressor</span></code></p>
<p>A random forest regressor.</p>
<p>A random forest is a meta estimator that fits a number of classifying
decision trees on various sub-samples of the dataset and uses averaging
to improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <cite>max_samples</cite> parameter if
<cite>bootstrap=True</cite> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;mse&quot;</em><em>, </em><em>&quot;mae&quot;}</em><em>, </em><em>default=&quot;mse&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion, and “mae” for the mean
absolute error.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls both the randomness of the bootstrapping of the samples used
when building trees (if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>) and the sampling of the
features to consider when looking for the best split at each node
(if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>).
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.DecisionTreeRegressor" title="EIMTC.models.DecisionTreeRegressor">DecisionTreeRegressor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeRegressor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.oob_score_" title="Permalink to this definition"></a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomForestRegressor.oob_prediction_">
<span class="sig-name descname"><span class="pre">oob_prediction_</span></span><a class="headerlink" href="#EIMTC.models.RandomForestRegressor.oob_prediction_" title="Permalink to this definition"></a></dt>
<dd><p>Prediction computed with out-of-bag estimate on the training set.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.DecisionTreeRegressor" title="EIMTC.models.DecisionTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.ExtraTreesRegressor" title="EIMTC.models.ExtraTreesRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreesRegressor</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>The default value <code class="docutils literal notranslate"><span class="pre">max_features=&quot;auto&quot;</span></code> uses <code class="docutils literal notranslate"><span class="pre">n_features</span></code>
rather than <code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">/</span> <span class="pre">3</span></code>. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id51"><span class="brackets">1</span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</p></li>
</ol>
</dd>
<dt class="label" id="id52"><span class="brackets">2</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-8.32987858]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">RandomTreesEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.BaseForest</span></code></p>
<p>An ensemble of totally random trees.</p>
<p>An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as there are trees in
the forest.</p>
<p>The dimensionality of the resulting representation is
<code class="docutils literal notranslate"><span class="pre">n_out</span> <span class="pre">&lt;=</span> <span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">max_leaf_nodes</span></code>. If <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">None</span></code>,
the number of leaf nodes is at most <code class="docutils literal notranslate"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>Number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=5</em>) – The maximum depth of each tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> is the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> is the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>sparse_output</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not to return a sparse CSR matrix, as default behavior,
or to return a dense array compatible with dense pipeline operators.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <a class="reference internal" href="#EIMTC.models.RandomTreesEmbedding.fit" title="EIMTC.models.RandomTreesEmbedding.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a>, <a class="reference internal" href="#EIMTC.models.RandomTreesEmbedding.transform" title="EIMTC.models.RandomTreesEmbedding.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform()</span></code></a>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the generation of the random <cite>y</cite> used to fit the trees
and the draw of the splits for each feature at the trees’ nodes.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.base_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>DecisionTreeClassifier instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier instances</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The feature importances (the higher, the more important the feature).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.one_hot_encoder_">
<span class="sig-name descname"><span class="pre">one_hot_encoder_</span></span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.one_hot_encoder_" title="Permalink to this definition"></a></dt>
<dd><p>One-hot encoder used to create the sparse embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>OneHotEncoder instance</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id53"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
<dt class="label" id="id54"><span class="brackets">2</span></dt>
<dd><p>Moosmann, F. and Triggs, B. and Jurie, F.  “Fast discriminative
visual codebooks using randomized clustering forests”
NIPS 2007</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomTreesEmbedding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_trees</span> <span class="o">=</span> <span class="n">RandomTreesEmbedding</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_sparse_embedding</span> <span class="o">=</span> <span class="n">random_trees</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_sparse_embedding</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],</span>
<span class="go">       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],</span>
<span class="go">       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.criterion">
<span class="sig-name descname"><span class="pre">criterion</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'mse'</span></em><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.criterion" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.max_features">
<span class="sig-name descname"><span class="pre">max_features</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">1</span></em><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.max_features" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for maximum
efficiency. Sparse matrices are also supported, use sparse
<code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code> for maximum efficiency.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.fit_transform">
<span class="sig-name descname"><span class="pre">fit_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.fit_transform" title="Permalink to this definition"></a></dt>
<dd><p>Fit estimator and transform dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data used to build forests. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for
maximum efficiency.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_transformed</strong> – Transformed dataset.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_out)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RandomTreesEmbedding.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RandomTreesEmbedding.transform" title="Permalink to this definition"></a></dt>
<dd><p>Transform dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data to be transformed. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for maximum
efficiency. Sparse matrices are also supported, use sparse
<code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code> for maximum efficiency.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_transformed</strong> – Transformed dataset.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_out)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">StackingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stack_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">passthrough</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._stacking._BaseStacking</span></code></p>
<p>Stack of estimators with a final classifier.</p>
<p>Stacked generalization consists in stacking the output of individual
estimator and use a classifier to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.</p>
<p>Note that <cite>estimators_</cite> are fitted on the full <cite>X</cite> while <cite>final_estimator_</cite>
is trained using cross-validated predictions of the base estimators using
<cite>cross_val_predict</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>)</em>) – Base estimators which will be stacked together. Each element of the
list is defined as a tuple of string (i.e. name) and an estimator
instance. An estimator can be set to ‘drop’ using <cite>set_params</cite>.</p></li>
<li><p><strong>final_estimator</strong> (<em>estimator</em><em>, </em><em>default=None</em>) – A classifier which will be used to combine the base estimators.
The default classifier is a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy used in
<cite>cross_val_predict</cite> to train <cite>final_estimator</cite>. Possible inputs for
cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross validation,</p></li>
<li><p>integer, to specify the number of folds in a (Stratified) KFold,</p></li>
<li><p>An object to be used as a cross-validation generator,</p></li>
<li><p>An iterable yielding train, test splits.</p></li>
</ul>
<p>For integer/None inputs, if the estimator is a classifier and y is
either binary or multiclass,
<code class="xref py py-class docutils literal notranslate"><span class="pre">StratifiedKFold</span></code> is used.
In all other cases, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.
These splitters are instantiated with <cite>shuffle=False</cite> so the splits
will be the same across calls.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A larger number of split will provide no benefits if the number
of training samples is large enough. Indeed, the training time
will increase. <code class="docutils literal notranslate"><span class="pre">cv</span></code> is not used for model evaluation but for
prediction.</p>
</div>
</p></li>
<li><p><strong>stack_method</strong> (<em>{'auto'</em><em>, </em><em>'predict_proba'</em><em>, </em><em>'decision_function'</em><em>, </em><em>'predict'}</em><em>,             </em><em>default='auto'</em>) – <p>Methods called for each base estimator. It can be:</p>
<ul>
<li><p>if ‘auto’, it will try to invoke, for each estimator,
<cite>‘predict_proba’</cite>, <cite>‘decision_function’</cite> or <cite>‘predict’</cite> in that
order.</p></li>
<li><p>otherwise, one of <cite>‘predict_proba’</cite>, <cite>‘decision_function’</cite> or
<cite>‘predict’</cite>. If the method is not implemented by the estimator, it
will raise an error.</p></li>
</ul>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel all <cite>estimators</cite> <cite>fit</cite>.
<cite>None</cite> means 1 unless in a <cite>joblib.parallel_backend</cite> context. -1 means
using all processors. See Glossary for more details.</p></li>
<li><p><strong>passthrough</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When False, only the predictions of estimators will be used as
training data for <cite>final_estimator</cite>. When True, the
<cite>final_estimator</cite> is trained on the predictions as well as the
original training data.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Verbosity level.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.StackingClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.StackingClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The elements of the estimators parameter, having been fitted on the
training data. If an estimator has been set to <cite>‘drop’</cite>, it
will not appear in <cite>estimators_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#EIMTC.models.StackingClassifier.named_estimators_" title="Permalink to this definition"></a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.final_estimator_">
<span class="sig-name descname"><span class="pre">final_estimator_</span></span><a class="headerlink" href="#EIMTC.models.StackingClassifier.final_estimator_" title="Permalink to this definition"></a></dt>
<dd><p>The classifier which predicts given the output of <cite>estimators_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.stack_method_">
<span class="sig-name descname"><span class="pre">stack_method_</span></span><a class="headerlink" href="#EIMTC.models.StackingClassifier.stack_method_" title="Permalink to this definition"></a></dt>
<dd><p>The method used by each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>When <cite>predict_proba</cite> is used by each estimator (i.e. most of the time for
<cite>stack_method=’auto’</cite> or specifically for <cite>stack_method=’predict_proba’</cite>),
The first column predicted by each estimator will be dropped in the case
of a binary classification problem. Indeed, both feature will be perfectly
collinear.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id55"><span class="brackets">1</span></dt>
<dd><p>Wolpert, David H. “Stacked generalization.” Neural networks 5.2
(1992): 241-259.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;svr&#39;</span><span class="p">,</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                          <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)))</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span> <span class="n">final_estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.9...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingClassifier.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">predict_params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict target for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>**predict_params</strong> (<em>dict of str -&gt; obj</em>) – Parameters to the <cite>predict</cite> called by the <cite>final_estimator</cite>. Note
that this may be used to return uncertainties from some estimators
with <cite>return_std</cite> or <cite>return_cov</cite>. Be aware that it will only
accounts for uncertainty in the final estimator.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_pred</strong> – Predicted targets.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,) or (n_samples, n_output)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities for X using
<cite>final_estimator_.predict_proba</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>probabilities</strong> – The class probabilities of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingClassifier.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Predict decision function for samples in X using
<cite>final_estimator_.decision_function</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>decisions</strong> – The decision function computed the final estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingClassifier.transform" title="Permalink to this definition"></a></dt>
<dd><p>Return class labels or probabilities for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_preds</strong> – Prediction outputs for each estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingClassifier.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#EIMTC.models.StackingClassifier.steps" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.StackingRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">StackingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">passthrough</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._stacking._BaseStacking</span></code></p>
<p>Stack of estimators with a final regressor.</p>
<p>Stacked generalization consists in stacking the output of individual
estimator and use a regressor to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.</p>
<p>Note that <cite>estimators_</cite> are fitted on the full <cite>X</cite> while <cite>final_estimator_</cite>
is trained using cross-validated predictions of the base estimators using
<cite>cross_val_predict</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>)</em>) – Base estimators which will be stacked together. Each element of the
list is defined as a tuple of string (i.e. name) and an estimator
instance. An estimator can be set to ‘drop’ using <cite>set_params</cite>.</p></li>
<li><p><strong>final_estimator</strong> (<em>estimator</em><em>, </em><em>default=None</em>) – A regressor which will be used to combine the base estimators.
The default regressor is a <code class="xref py py-class docutils literal notranslate"><span class="pre">RidgeCV</span></code>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy used in
<cite>cross_val_predict</cite> to train <cite>final_estimator</cite>. Possible inputs for
cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross validation,</p></li>
<li><p>integer, to specify the number of folds in a (Stratified) KFold,</p></li>
<li><p>An object to be used as a cross-validation generator,</p></li>
<li><p>An iterable yielding train, test splits.</p></li>
</ul>
<p>For integer/None inputs, if the estimator is a classifier and y is
either binary or multiclass,
<code class="xref py py-class docutils literal notranslate"><span class="pre">StratifiedKFold</span></code> is used.
In all other cases, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.
These splitters are instantiated with <cite>shuffle=False</cite> so the splits
will be the same across calls.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A larger number of split will provide no benefits if the number
of training samples is large enough. Indeed, the training time
will increase. <code class="docutils literal notranslate"><span class="pre">cv</span></code> is not used for model evaluation but for
prediction.</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for <cite>fit</cite> of all <cite>estimators</cite>.
<cite>None</cite> means 1 unless in a <cite>joblib.parallel_backend</cite> context. -1 means
using all processors. See Glossary for more details.</p></li>
<li><p><strong>passthrough</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When False, only the predictions of estimators will be used as
training data for <cite>final_estimator</cite>. When True, the
<cite>final_estimator</cite> is trained on the predictions as well as the
original training data.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Verbosity level.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.StackingRegressor.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The elements of the estimators parameter, having been fitted on the
training data. If an estimator has been set to <cite>‘drop’</cite>, it
will not appear in <cite>estimators_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingRegressor.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#EIMTC.models.StackingRegressor.named_estimators_" title="Permalink to this definition"></a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="simple">
<dt><a href="#id84"><span class="problematic" id="id85">final_estimator_</span></a><span class="classifier">estimator</span></dt><dd><p>The regressor to stacked the base estimators fitted.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id56"><span class="brackets">1</span></dt>
<dd><p>Wolpert, David H. “Stacked generalization.” Neural networks 5.2
(1992): 241-259.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;svr&#39;</span><span class="p">,</span> <span class="n">LinearSVR</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>                                          <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.3...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingRegressor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.StackingRegressor.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.StackingRegressor.transform" title="Permalink to this definition"></a></dt>
<dd><p>Return the predictions for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_preds</strong> – Prediction outputs for each estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_estimators)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.StackingRegressor.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#EIMTC.models.StackingRegressor.steps" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">VotingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">voting</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hard'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._voting._BaseVoting</span></code></p>
<p>Soft Voting/Majority Rule classifier for unfitted estimators.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>) </em><em>tuples</em>) – <p>Invoking the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the <code class="docutils literal notranslate"><span class="pre">VotingClassifier</span></code> will fit clones
of those original estimators that will be stored in the class attribute
<code class="docutils literal notranslate"><span class="pre">self.estimators_</span></code>. An estimator can be set to <code class="docutils literal notranslate"><span class="pre">'drop'</span></code>
using <code class="docutils literal notranslate"><span class="pre">set_params</span></code>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.21: </span><code class="docutils literal notranslate"><span class="pre">'drop'</span></code> is accepted. Using None was deprecated in 0.22 and
support was removed in 0.24.</p>
</div>
</p></li>
<li><p><strong>voting</strong> (<em>{'hard'</em><em>, </em><em>'soft'}</em><em>, </em><em>default='hard'</em>) – If ‘hard’, uses predicted class labels for majority rule voting.
Else if ‘soft’, predicts the class label based on the argmax of
the sums of the predicted probabilities, which is recommended for
an ensemble of well-calibrated classifiers.</p></li>
<li><p><strong>weights</strong> (<em>array-like of shape</em><em> (</em><em>n_classifiers</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sequence of weights (<cite>float</cite> or <cite>int</cite>) to weight the occurrences of
predicted class labels (<cite>hard</cite> voting) or class probabilities
before averaging (<cite>soft</cite> voting). Uses uniform weights if <cite>None</cite>.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to run in parallel for <code class="docutils literal notranslate"><span class="pre">fit</span></code>.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
<li><p><strong>flatten_transform</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Affects shape of transform output only when voting=’soft’
If voting=’soft’ and flatten_transform=True, transform method returns
matrix with shape (n_samples, n_classifiers * n_classes). If
flatten_transform=False, it returns
(n_classifiers, n_samples, n_classes).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If True, the time elapsed while fitting will be printed as it
is completed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.VotingClassifier.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators as defined in <code class="docutils literal notranslate"><span class="pre">estimators</span></code>
that are not ‘drop’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of classifiers</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#EIMTC.models.VotingClassifier.named_estimators_" title="Permalink to this definition"></a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.VotingClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_predictions,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.VotingRegressor" title="EIMTC.models.VotingRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VotingRegressor</span></code></a></dt><dd><p>Prediction voting regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">VotingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">eclf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">named_estimators_</span><span class="o">.</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
<span class="gp">... </span>               <span class="n">eclf1</span><span class="o">.</span><span class="n">named_estimators_</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">eclf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>       <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>       <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>       <span class="n">flatten_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">eclf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(6, 6)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingClassifier.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict class labels for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>maj</strong> – Predicted class labels.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.predict_proba">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">predict_proba</span></span><a class="headerlink" href="#EIMTC.models.VotingClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Compute probabilities of possible outcomes for samples in X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>avg</strong> – Weighted average probability for each class per sample.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingClassifier.transform" title="Permalink to this definition"></a></dt>
<dd><p>Return class labels or probabilities for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>If <cite>voting=’soft’</cite> and <cite>flatten_transform=True</cite>:</dt><dd><p>returns ndarray of shape (n_classifiers, n_samples *
n_classes), being class probabilities calculated by each
classifier.</p>
</dd>
<dt>If <cite>voting=’soft’ and `flatten_transform=False</cite>:</dt><dd><p>ndarray of shape (n_classifiers, n_samples, n_classes)</p>
</dd>
<dt>If <cite>voting=’hard’</cite>:</dt><dd><p>ndarray of shape (n_samples, n_classifiers), being
class labels predicted by each classifier.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>probabilities_or_labels</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingClassifier.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#EIMTC.models.VotingClassifier.steps" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">VotingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._voting._BaseVoting</span></code></p>
<p>Prediction voting regressor for unfitted estimators.</p>
<p>A voting regressor is an ensemble meta-estimator that fits several base
regressors, each on the whole dataset. Then it averages the individual
predictions to form a final prediction.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>) </em><em>tuples</em>) – <p>Invoking the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the <code class="docutils literal notranslate"><span class="pre">VotingRegressor</span></code> will fit clones
of those original estimators that will be stored in the class attribute
<code class="docutils literal notranslate"><span class="pre">self.estimators_</span></code>. An estimator can be set to <code class="docutils literal notranslate"><span class="pre">'drop'</span></code> using
<code class="docutils literal notranslate"><span class="pre">set_params</span></code>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.21: </span><code class="docutils literal notranslate"><span class="pre">'drop'</span></code> is accepted. Using None was deprecated in 0.22 and
support was removed in 0.24.</p>
</div>
</p></li>
<li><p><strong>weights</strong> (<em>array-like of shape</em><em> (</em><em>n_regressors</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sequence of weights (<cite>float</cite> or <cite>int</cite>) to weight the occurrences of
predicted values before averaging. Uses uniform weights if <cite>None</cite>.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for <code class="docutils literal notranslate"><span class="pre">fit</span></code>.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If True, the time elapsed while fitting will be printed as it
is completed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#EIMTC.models.VotingRegressor.estimators_" title="Permalink to this definition"></a></dt>
<dd><p>The collection of fitted sub-estimators as defined in <code class="docutils literal notranslate"><span class="pre">estimators</span></code>
that are not ‘drop’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of regressors</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#EIMTC.models.VotingRegressor.named_estimators_" title="Permalink to this definition"></a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Bunch</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.VotingClassifier" title="EIMTC.models.VotingClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a></dt><dd><p>Soft Voting/Majority Rule classifier.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r1</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">36</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">42</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">er</span> <span class="o">=</span> <span class="n">VotingRegressor</span><span class="p">([(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">r1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">r2</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[ 3.3  5.7 11.8 19.7 28.  40.3]</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingRegressor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingRegressor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict regression target for X.</p>
<p>The predicted regression target of an input sample is computed as the
mean predicted regression targets of the estimators in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.VotingRegressor.transform" title="Permalink to this definition"></a></dt>
<dd><p>Return predictions for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>predictions</strong> – Values predicted by each regressor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classifiers)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.VotingRegressor.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#EIMTC.models.VotingRegressor.steps" title="Permalink to this definition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">KNeighborsClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.KNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Classifier implementing the k-nearest neighbors vote.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>default=5</em>) – Number of neighbors to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.</p></li>
<li><p><strong>weights</strong> (<em>{'uniform'</em><em>, </em><em>'distance'}</em><em> or </em><em>callable</em><em>, </em><em>default='uniform'</em>) – <p>weight function used in prediction.  Possible values:</p>
<ul>
<li><p>’uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</p></li>
<li><p>’distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</p></li>
<li><p>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</p></li>
</ul>
</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.KNeighborsClassifier.fit" title="EIMTC.models.KNeighborsClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of <a class="reference internal" href="#EIMTC.models.DistanceMetric" title="EIMTC.models.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a> for a
list of available metrics.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a <span class="xref std std-term">sparse graph</span>,
in which case only “nonzero” elements may be considered neighbors.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.
Doesn’t affect <a class="reference internal" href="#EIMTC.models.KNeighborsClassifier.fit" title="EIMTC.models.KNeighborsClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels known to the classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The distance metric used. It will be same as the <cite>metric</cite> parameter
or a synonym of it, e.g. ‘euclidean’ if the <cite>metric</cite> parameter set to
‘minkowski’ and <cite>p</cite> parameter set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str or callble</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Additional keyword arguments for the metric function. For most metrics
will be same with <cite>metric_params</cite> parameter, but may also contain the
<cite>p</cite> parameter value if the <cite>effective_metric_</cite> attribute is set to
‘minkowski’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.outputs_2d_">
<span class="sig-name descname"><span class="pre">outputs_2d_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.outputs_2d_" title="Permalink to this definition"></a></dt>
<dd><p>False when <cite>y</cite>’s shape is (n_samples, ) or (n_samples, 1) during fit
otherwise True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.1</span><span class="p">]]))</span>
<span class="go">[0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">]]))</span>
<span class="go">[[0.66666667 0.33333333]]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.RadiusNeighborsClassifier" title="EIMTC.models.RadiusNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.KNeighborsRegressor" title="EIMTC.models.KNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsRegressor" title="EIMTC.models.RadiusNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.NearestNeighbors" title="EIMTC.models.NearestNeighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> and <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances
but different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the k-nearest neighbors classifier from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted k-nearest neighbors classifier.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.KNeighborsClassifier" title="EIMTC.models.KNeighborsClassifier">KNeighborsClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict the class labels for the provided data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_queries</em><em>, </em><em>n_features</em><em>)</em><em>,                 or </em><em>(</em><em>n_queries</em><em>, </em><em>n_indexed</em><em>) </em><em>if metric == 'precomputed'</em>) – Test samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – Class labels for each data sample.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_queries,) or (n_queries, n_outputs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Return probability estimates for the test data X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_queries</em><em>, </em><em>n_features</em><em>)</em><em>,                 or </em><em>(</em><em>n_queries</em><em>, </em><em>n_indexed</em><em>) </em><em>if metric == 'precomputed'</em>) – Test samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – of such arrays if n_outputs &gt; 1.
The class probabilities of the input samples. Classes are ordered
by lexicographic order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_queries, n_classes), or a list of n_outputs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">KNeighborsRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.KNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Regression based on k-nearest neighbors.</p>
<p>The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>default=5</em>) – Number of neighbors to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.</p></li>
<li><p><strong>weights</strong> (<em>{'uniform'</em><em>, </em><em>'distance'}</em><em> or </em><em>callable</em><em>, </em><em>default='uniform'</em>) – <p>weight function used in prediction.  Possible values:</p>
<ul>
<li><p>’uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</p></li>
<li><p>’distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</p></li>
<li><p>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</p></li>
</ul>
<p>Uniform weights are used by default.</p>
</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.KNeighborsRegressor.fit" title="EIMTC.models.KNeighborsRegressor.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of <a class="reference internal" href="#EIMTC.models.DistanceMetric" title="EIMTC.models.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a> for a
list of available metrics.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a <span class="xref std std-term">sparse graph</span>,
in which case only “nonzero” elements may be considered neighbors.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.
Doesn’t affect <a class="reference internal" href="#EIMTC.models.KNeighborsRegressor.fit" title="EIMTC.models.KNeighborsRegressor.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsRegressor.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsRegressor.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The distance metric to use. It will be same as the <cite>metric</cite> parameter
or a synonym of it, e.g. ‘euclidean’ if the <cite>metric</cite> parameter set to
‘minkowski’ and <cite>p</cite> parameter set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str or callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsRegressor.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsRegressor.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Additional keyword arguments for the metric function. For most metrics
will be same with <cite>metric_params</cite> parameter, but may also contain the
<cite>p</cite> parameter value if the <cite>effective_metric_</cite> attribute is set to
‘minkowski’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsRegressor.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsRegressor.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">KNeighborsRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0.5]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.NearestNeighbors" title="EIMTC.models.NearestNeighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsRegressor" title="EIMTC.models.RadiusNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.KNeighborsClassifier" title="EIMTC.models.KNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsClassifier" title="EIMTC.models.RadiusNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> and <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Regarding the Nearest Neighbors algorithms, if it is found that two
neighbors, neighbor <cite>k+1</cite> and <cite>k</cite>, have identical distances but
different labels, the results will depend on the ordering of the
training data.</p>
</div>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsRegressor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the k-nearest neighbors regressor from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted k-nearest neighbors regressor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.KNeighborsRegressor" title="EIMTC.models.KNeighborsRegressor">KNeighborsRegressor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsRegressor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict the target for the provided data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_queries</em><em>, </em><em>n_features</em><em>)</em><em>,                 or </em><em>(</em><em>n_queries</em><em>, </em><em>n_indexed</em><em>) </em><em>if metric == 'precomputed'</em>) – Test samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – Target values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">KNeighborsTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'distance'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.KNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.TransformerMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Transform X into a (weighted) graph of k nearest neighbors</p>
<p>The transformed data is a sparse graph as returned by kneighbors_graph.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<em>{'distance'</em><em>, </em><em>'connectivity'}</em><em>, </em><em>default='distance'</em>) – Type of returned matrix: ‘connectivity’ will return the connectivity
matrix with ones and zeros, and ‘distance’ will return the distances
between neighbors according to the given metric.</p></li>
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>default=5</em>) – Number of neighbors for each sample in the transformed sparse graph.
For compatibility reasons, as each sample is considered as its own
neighbor, one extra neighbor will be computed when mode == ‘distance’.
In this case, the sparse graph contains (n_neighbors + 1) neighbors.</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.KNeighborsTransformer.fit" title="EIMTC.models.KNeighborsTransformer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – <p>metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=1</em>) – The number of parallel jobs to run for neighbors search.
If <code class="docutils literal notranslate"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The distance metric used. It will be same as the <cite>metric</cite> parameter
or a synonym of it, e.g. ‘euclidean’ if the <cite>metric</cite> parameter set to
‘minkowski’ and <cite>p</cite> parameter set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str or callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Additional keyword arguments for the metric function. For most metrics
will be same with <cite>metric_params</cite> parameter, but may also contain the
<cite>p</cite> parameter value if the <cite>effective_metric_</cite> attribute is set to
‘minkowski’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">KNeighborsTransformer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">Isomap</span><span class="p">(</span><span class="n">neighbors_algorithm</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the k-nearest neighbors transformer from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted k-nearest neighbors transformer.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.KNeighborsTransformer" title="EIMTC.models.KNeighborsTransformer">KNeighborsTransformer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer.transform" title="Permalink to this definition"></a></dt>
<dd><p>Computes the (weighted) graph of Neighbors for points in X</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_transform</em><em>, </em><em>n_features</em><em>)</em>) – Sample data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Xt</strong> – Xt[i, j] is assigned the weight of edge that connects i to j.
Only the neighbors have an explicit value.
The diagonal is always explicit.
The matrix is of CSR format.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples_transform, n_samples_fit)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.KNeighborsTransformer.fit_transform">
<span class="sig-name descname"><span class="pre">fit_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KNeighborsTransformer.fit_transform" title="Permalink to this definition"></a></dt>
<dd><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training set.</p></li>
<li><p><strong>y</strong> (<em>ignored</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Xt</strong> – Xt[i, j] is assigned the weight of edge that connects i to j.
Only the neighbors have an explicit value.
The diagonal is always explicit.
The matrix is of CSR format.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_samples)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">LocalOutlierFactor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">novelty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.KNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.OutlierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Unsupervised Outlier Detection using Local Outlier Factor (LOF)</p>
<p>The anomaly score of each sample is called Local Outlier Factor.
It measures the local deviation of density of a given sample with
respect to its neighbors.
It is local in that the anomaly score depends on how isolated the object
is with respect to the surrounding neighborhood.
More precisely, locality is given by k-nearest neighbors, whose distance
is used to estimate the local density.
By comparing the local density of a sample to the local densities of
its neighbors, one can identify samples that have a substantially lower
density than their neighbors. These are considered outliers.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>default=20</em>) – Number of neighbors to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.
If n_neighbors is larger than the number of samples provided,
all samples will be used.</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.LocalOutlierFactor.fit" title="EIMTC.models.LocalOutlierFactor.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> or <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a>. This can
affect the speed of the construction and query, as well as the memory
required to store the tree. The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – <p>metric used for the distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is “precomputed”, X is assumed to be a distance matrix and
must be square. X may be a sparse matrix, in which case only “nonzero”
elements may be considered neighbors.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics:
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/spatial.distance.html">https://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a></p>
</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Parameter for the Minkowski metric from
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_distances()</span></code>. When p = 1, this
is equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>contamination</strong> (<em>'auto'</em><em> or </em><em>float</em><em>, </em><em>default='auto'</em>) – <p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. When fitting this is used to define the
threshold on the scores of the samples.</p>
<ul>
<li><p>if ‘auto’, the threshold is determined as in the
original paper,</p></li>
<li><p>if a float, the contamination should be in the range [0, 0.5].</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">contamination</span></code> changed from 0.1
to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>.</p>
</div>
</p></li>
<li><p><strong>novelty</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>By default, LocalOutlierFactor is only meant to be used for outlier
detection (novelty=False). Set novelty to True if you want to use
LocalOutlierFactor for novelty detection. In this case be aware that
you should only use predict, decision_function and score_samples
on new unseen data and not on the training set.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.negative_outlier_factor_">
<span class="sig-name descname"><span class="pre">negative_outlier_factor_</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.negative_outlier_factor_" title="Permalink to this definition"></a></dt>
<dd><p>The opposite LOF of the training samples. The higher, the more normal.
Inliers tend to have a LOF score close to 1
(<code class="docutils literal notranslate"><span class="pre">negative_outlier_factor_</span></code> close to -1), while outliers tend to have
a larger LOF score.</p>
<p>The local outlier factor (LOF) of a sample captures its
supposed ‘degree of abnormality’.
It is the average of the ratio of the local reachability density of
a sample and those of its k-nearest neighbors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.n_neighbors_">
<span class="sig-name descname"><span class="pre">n_neighbors_</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.n_neighbors_" title="Permalink to this definition"></a></dt>
<dd><p>The actual number of neighbors used for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.offset_">
<span class="sig-name descname"><span class="pre">offset_</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.offset_" title="Permalink to this definition"></a></dt>
<dd><p>Offset used to obtain binary labels from the raw scores.
Observations having a negative_outlier_factor smaller than <cite>offset_</cite>
are detected as abnormal.
The offset is set to -1.5 (inliers score around -1), except when a
contamination parameter different than “auto” is provided. In that
case, the offset is defined in such a way we obtain the expected
number of outliers in training.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The effective metric used for the distance computation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>The effective additional keyword arguments for the metric function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>It is the number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">LocalOutlierFactor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">101.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LocalOutlierFactor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([ 1,  1, -1,  1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">negative_outlier_factor_</span>
<span class="go">array([ -0.9821...,  -1.0370..., -73.3697...,  -0.9821...])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id57"><span class="brackets">1</span></dt>
<dd><p>Breunig, M. M., Kriegel, H. P., Ng, R. T., &amp; Sander, J. (2000, May).
LOF: identifying density-based local outliers. In ACM sigmod record.</p>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.fit_predict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fit_predict</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.fit_predict" title="Permalink to this definition"></a></dt>
<dd><p>Fits the model to the training set X and returns the labels.</p>
<p><strong>Not available for novelty detection (when novelty is set to True).</strong>
Label is 1 for an inlier and -1 for an outlier according to the LOF
score and the contamination parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – The query sample or samples to compute the Local Outlier Factor
w.r.t. to the training samples.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>is_inlier</strong> – Returns -1 for anomalies/outliers and 1 for inliers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the local outlier factor detector from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted local outlier factor detector.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.LocalOutlierFactor" title="EIMTC.models.LocalOutlierFactor">LocalOutlierFactor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.predict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">predict</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict the labels (1 inlier, -1 outlier) of X according to LOF.</p>
<p><strong>Only available for novelty detection (when novelty is set to True).</strong>
This method allows to generalize prediction to <em>new observations</em> (not
in the training set).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The query sample or samples to compute the Local Outlier Factor
w.r.t. to the training samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>is_inlier</strong> – Returns -1 for anomalies/outliers and +1 for inliers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.decision_function">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">decision_function</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.decision_function" title="Permalink to this definition"></a></dt>
<dd><p>Shifted opposite of the Local Outlier Factor of X.</p>
<p>Bigger is better, i.e. large values correspond to inliers.</p>
<p><strong>Only available for novelty detection (when novelty is set to True).</strong>
The shift offset allows a zero threshold for being an outlier.
The argument X is supposed to contain <em>new data</em>: if X contains a
point from training, it considers the later in its own neighborhood.
Also, the samples in X are not considered in the neighborhood of any
point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The query sample or samples to compute the Local Outlier Factor
w.r.t. the training samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>shifted_opposite_lof_scores</strong> – The shifted opposite of the Local Outlier Factor of each input
samples. The lower, the more abnormal. Negative scores represent
outliers, positive scores represent inliers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="EIMTC.models.LocalOutlierFactor.score_samples">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">score_samples</span></span><a class="headerlink" href="#EIMTC.models.LocalOutlierFactor.score_samples" title="Permalink to this definition"></a></dt>
<dd><p>Opposite of the Local Outlier Factor of X.</p>
<p>It is the opposite as bigger is better, i.e. large values correspond
to inliers.</p>
<p><strong>Only available for novelty detection (when novelty is set to True).</strong>
The argument X is supposed to contain <em>new data</em>: if X contains a
point from training, it considers the later in its own neighborhood.
Also, the samples in X are not considered in the neighborhood of any
point.
The score_samples on training data is available by considering the
the <code class="docutils literal notranslate"><span class="pre">negative_outlier_factor_</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The query sample or samples to compute the Local Outlier Factor
w.r.t. the training samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>opposite_lof_scores</strong> – The opposite of the Local Outlier Factor of each input samples.
The lower, the more abnormal.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.NearestCentroid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">NearestCentroid</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shrink_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NearestCentroid" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Nearest centroid classifier.</p>
<p>Each class is represented by its centroid, with test samples classified to
the class with the nearest centroid.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em>) – <p>The metric to use when calculating distance between instances in a
feature array. If metric is a string or callable, it must be one of
the options allowed by metrics.pairwise.pairwise_distances for its
metric parameter.
The centroids for the samples corresponding to each class is the point
from which the sum of the distances (according to the metric) of all
samples that belong to that particular class are minimized.
If the “manhattan” metric is provided, this centroid is the median and
for all other metrics, the centroid is now set to be the mean.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span><code class="docutils literal notranslate"><span class="pre">metric='precomputed'</span></code> was deprecated and now raises an error</p>
</div>
</p></li>
<li><p><strong>shrink_threshold</strong> (<em>float</em><em>, </em><em>default=None</em>) – Threshold for shrinking centroids to remove features.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NearestCentroid.centroids_">
<span class="sig-name descname"><span class="pre">centroids_</span></span><a class="headerlink" href="#EIMTC.models.NearestCentroid.centroids_" title="Permalink to this definition"></a></dt>
<dd><p>Centroid of each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NearestCentroid.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.NearestCentroid.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The unique classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.KNeighborsClassifier" title="EIMTC.models.KNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a></dt><dd><p>Nearest neighbors classifier.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>When used for text classification with tf-idf vectors, this classifier is
also known as the Rocchio classifier.</p>
<p class="rubric">References</p>
<p>Tibshirani, R., Hastie, T., Narasimhan, B., &amp; Chu, G. (2002). Diagnosis of
multiple cancer types by shrunken centroids of gene expression. Proceedings
of the National Academy of Sciences of the United States of America,
99(10), 6567-6572. The National Academy of Sciences.</p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.NearestCentroid.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NearestCentroid.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the NearestCentroid model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples is the number of samples and
n_features is the number of features.
Note that centroid shrinking cannot be used with sparse matrices.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values (integers)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.NearestCentroid.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NearestCentroid.predict" title="Permalink to this definition"></a></dt>
<dd><p>Perform classification on an array of test vectors X.</p>
<p>The predicted class C for each sample in X is returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>C</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If the metric constructor parameter is “precomputed”, X is assumed to
be the distance matrix between the data to be predicted and
<code class="docutils literal notranslate"><span class="pre">self.centroids_</span></code>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.NearestNeighbors">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">NearestNeighbors</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_neighbors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NearestNeighbors" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.KNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.RadiusNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Unsupervised learner for implementing neighbor searches.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_neighbors</strong> (<em>int</em><em>, </em><em>default=5</em>) – Number of neighbors to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">kneighbors()</span></code> queries.</p></li>
<li><p><strong>radius</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Range of parameter space to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">radius_neighbors()</span></code>
queries.</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.NearestNeighbors.fit" title="EIMTC.models.NearestNeighbors.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of <a class="reference internal" href="#EIMTC.models.DistanceMetric" title="EIMTC.models.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a> for a
list of available metrics.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a <span class="xref std std-term">sparse graph</span>,
in which case only “nonzero” elements may be considered neighbors.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NearestNeighbors.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.NearestNeighbors.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>Metric used to compute distances to neighbors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NearestNeighbors.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.NearestNeighbors.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Parameters for the metric used to compute distances to neighbors.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NearestNeighbors.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.NearestNeighbors.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">samples</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="go">NearestNeighbors(...)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">]],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">array([[2, 0]]...)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span> <span class="o">=</span> <span class="n">neigh</span><span class="o">.</span><span class="n">radius_neighbors</span><span class="p">(</span>
<span class="gp">... </span>   <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">]],</span> <span class="mf">0.4</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">False</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">nbrs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="go">array(2)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.KNeighborsClassifier" title="EIMTC.models.KNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsClassifier" title="EIMTC.models.RadiusNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.KNeighborsRegressor" title="EIMTC.models.KNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsRegressor" title="EIMTC.models.RadiusNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BallTree</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> and <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.NearestNeighbors.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NearestNeighbors.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the nearest neighbors estimator from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted nearest neighbors estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.NearestNeighbors" title="EIMTC.models.NearestNeighbors">NearestNeighbors</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.NeighborhoodComponentsAnalysis">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">NeighborhoodComponentsAnalysis</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NeighborhoodComponentsAnalysis" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.TransformerMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Neighborhood Components Analysis</p>
<p>Neighborhood Component Analysis (NCA) is a machine learning algorithm for
metric learning. It learns a linear transformation in a supervised fashion
to improve the classification accuracy of a stochastic nearest neighbors
rule in the transformed space.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<em>int</em><em>, </em><em>default=None</em>) – Preferred dimensionality of the projected space.
If None it will be set to <code class="docutils literal notranslate"><span class="pre">n_features</span></code>.</p></li>
<li><p><strong>init</strong> (<em>{'auto'</em><em>, </em><em>'pca'</em><em>, </em><em>'lda'</em><em>, </em><em>'identity'</em><em>, </em><em>'random'}</em><em> or </em><em>ndarray of shape</em><em>             (</em><em>n_features_a</em><em>, </em><em>n_features_b</em><em>)</em><em>, </em><em>default='auto'</em>) – <p>Initialization of the linear transformation. Possible options are
‘auto’, ‘pca’, ‘lda’, ‘identity’, ‘random’, and a numpy array of shape
(n_features_a, n_features_b).</p>
<dl class="simple">
<dt>’auto’</dt><dd><p>Depending on <code class="docutils literal notranslate"><span class="pre">n_components</span></code>, the most reasonable initialization
will be chosen. If <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">&lt;=</span> <span class="pre">n_classes</span></code> we use ‘lda’, as
it uses labels information. If not, but
<code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">&lt;</span> <span class="pre">min(n_features,</span> <span class="pre">n_samples)</span></code>, we use ‘pca’, as
it projects data in meaningful directions (those of higher
variance). Otherwise, we just use ‘identity’.</p>
</dd>
<dt>’pca’</dt><dd><p><code class="docutils literal notranslate"><span class="pre">n_components</span></code> principal components of the inputs passed
to <a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> will be used to initialize the transformation.
(See <code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code>)</p>
</dd>
<dt>’lda’</dt><dd><p><code class="docutils literal notranslate"><span class="pre">min(n_components,</span> <span class="pre">n_classes)</span></code> most discriminative
components of the inputs passed to <a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> will be used to
initialize the transformation. (If <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">&gt;</span> <span class="pre">n_classes</span></code>,
the rest of the components will be zero.) (See
<code class="xref py py-class docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code>)</p>
</dd>
<dt>’identity’</dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is strictly smaller than the
dimensionality of the inputs passed to <a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a>, the identity
matrix will be truncated to the first <code class="docutils literal notranslate"><span class="pre">n_components</span></code> rows.</p>
</dd>
<dt>’random’</dt><dd><p>The initial transformation will be a random array of shape
<cite>(n_components, n_features)</cite>. Each value is sampled from the
standard normal distribution.</p>
</dd>
<dt>numpy array</dt><dd><p>n_features_b must match the dimensionality of the inputs passed to
<a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> and n_features_a must be less than or equal to that.
If <code class="docutils literal notranslate"><span class="pre">n_components</span></code> is not None, n_features_a must match it.</p>
</dd>
</dl>
</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If True and <a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> has been called before, the solution of the
previous call to <a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> is used as the initial linear
transformation (<code class="docutils literal notranslate"><span class="pre">n_components</span></code> and <code class="docutils literal notranslate"><span class="pre">init</span></code> will be ignored).</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=50</em>) – Maximum number of iterations in the optimization.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-5</em>) – Convergence tolerance for the optimization.</p></li>
<li><p><strong>callback</strong> (<em>callable</em><em>, </em><em>default=None</em>) – If not None, this function is called after every iteration of the
optimizer, taking as arguments the current solution (flattened
transformation matrix) and the number of iterations. This might be
useful in case one wants to examine or store the transformation
found after each iteration.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – If 0, no progress messages will be printed.
If 1, progress messages will be printed to stdout.
If &gt; 1, progress messages will be printed and the <code class="docutils literal notranslate"><span class="pre">disp</span></code>
parameter of <code class="xref py py-func docutils literal notranslate"><span class="pre">scipy.optimize.minimize()</span></code> will be set to
<code class="docutils literal notranslate"><span class="pre">verbose</span> <span class="pre">-</span> <span class="pre">2</span></code>.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em> or </em><em>numpy.RandomState</em><em>, </em><em>default=None</em>) – A pseudo random number generator object or a seed for it if int. If
<code class="docutils literal notranslate"><span class="pre">init='random'</span></code>, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> is used to initialize the random
transformation. If <code class="docutils literal notranslate"><span class="pre">init='pca'</span></code>, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> is passed as an
argument to PCA when initializing the transformation. Pass an int
for reproducible results across multiple function calls.
See :term: <cite>Glossary &lt;random_state&gt;</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NeighborhoodComponentsAnalysis.components_">
<span class="sig-name descname"><span class="pre">components_</span></span><a class="headerlink" href="#EIMTC.models.NeighborhoodComponentsAnalysis.components_" title="Permalink to this definition"></a></dt>
<dd><p>The linear transformation learned during fitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_components, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NeighborhoodComponentsAnalysis.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#EIMTC.models.NeighborhoodComponentsAnalysis.n_iter_" title="Permalink to this definition"></a></dt>
<dd><p>Counts the number of iterations performed by the optimizer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.NeighborhoodComponentsAnalysis.random_state_">
<span class="sig-name descname"><span class="pre">random_state_</span></span><a class="headerlink" href="#EIMTC.models.NeighborhoodComponentsAnalysis.random_state_" title="Permalink to this definition"></a></dt>
<dd><p>Pseudo random number generator object used during initialization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>numpy.RandomState</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NeighborhoodComponentsAnalysis</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<span class="gp">... </span><span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nca</span> <span class="o">=</span> <span class="n">NeighborhoodComponentsAnalysis</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">NeighborhoodComponentsAnalysis(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="go">0.933333...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">nca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">KNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">nca</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">))</span>
<span class="go">0.961904...</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id58"><span class="brackets">1</span></dt>
<dd><p>J. Goldberger, G. Hinton, S. Roweis, R. Salakhutdinov.
“Neighbourhood Components Analysis”. Advances in Neural Information
Processing Systems. 17, 513-520, 2005.
<a class="reference external" href="http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf">http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf</a></p>
</dd>
<dt class="label" id="id59"><span class="brackets">2</span></dt>
<dd><p>Wikipedia entry on Neighborhood Components Analysis
<a class="reference external" href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis">https://en.wikipedia.org/wiki/Neighbourhood_components_analysis</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.NeighborhoodComponentsAnalysis.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training samples.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The corresponding training labels.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – returns a trained NeighborhoodComponentsAnalysis model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.NeighborhoodComponentsAnalysis.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.NeighborhoodComponentsAnalysis.transform" title="Permalink to this definition"></a></dt>
<dd><p>Applies the learned transformation to the given data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Data samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_embedded</strong> – The data samples transformed.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_components)</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>NotFittedError</strong> – If <a class="reference internal" href="#EIMTC.models.NeighborhoodComponentsAnalysis.fit" title="EIMTC.models.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> has not been called before.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">RadiusNeighborsClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outlier_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.RadiusNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Classifier implementing a vote among neighbors within a given radius</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>radius</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Range of parameter space to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">radius_neighbors()</span></code>
queries.</p></li>
<li><p><strong>weights</strong> (<em>{'uniform'</em><em>, </em><em>'distance'}</em><em> or </em><em>callable</em><em>, </em><em>default='uniform'</em>) – <p>weight function used in prediction.  Possible values:</p>
<ul>
<li><p>’uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</p></li>
<li><p>’distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</p></li>
<li><p>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</p></li>
</ul>
<p>Uniform weights are used by default.</p>
</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.RadiusNeighborsClassifier.fit" title="EIMTC.models.RadiusNeighborsClassifier.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of <a class="reference internal" href="#EIMTC.models.DistanceMetric" title="EIMTC.models.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a> for a
list of available metrics.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a <span class="xref std std-term">sparse graph</span>,
in which case only “nonzero” elements may be considered neighbors.</p></li>
<li><p><strong>outlier_label</strong> (<em>{manual label</em><em>, </em><em>'most_frequent'}</em><em>, </em><em>default=None</em>) – <p>label for outlier samples (samples with no neighbors in given radius).</p>
<ul>
<li><p>manual label: str or int label (should be the same type as y)
or list of manual labels if multi-output is used.</p></li>
<li><p>’most_frequent’ : assign the most frequent label of y to outliers.</p></li>
<li><p>None : when any outlier is detected, ValueError will be raised.</p></li>
</ul>
</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels known to the classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The distance metric used. It will be same as the <cite>metric</cite> parameter
or a synonym of it, e.g. ‘euclidean’ if the <cite>metric</cite> parameter set to
‘minkowski’ and <cite>p</cite> parameter set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str or callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Additional keyword arguments for the metric function. For most metrics
will be same with <cite>metric_params</cite> parameter, but may also contain the
<cite>p</cite> parameter value if the <cite>effective_metric_</cite> attribute is set to
‘minkowski’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.outlier_label_">
<span class="sig-name descname"><span class="pre">outlier_label_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.outlier_label_" title="Permalink to this definition"></a></dt>
<dd><p>Label which is given for outlier samples (samples with no neighbors
on given radius).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or array-like of shape (n_class,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.outputs_2d_">
<span class="sig-name descname"><span class="pre">outputs_2d_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.outputs_2d_" title="Permalink to this definition"></a></dt>
<dd><p>False when <cite>y</cite>’s shape is (n_samples, ) or (n_samples, 1) during fit
otherwise True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsClassifier</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RadiusNeighborsClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">]]))</span>
<span class="go">[[0.66666667 0.33333333]]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.KNeighborsClassifier" title="EIMTC.models.KNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsRegressor" title="EIMTC.models.RadiusNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.KNeighborsRegressor" title="EIMTC.models.KNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.NearestNeighbors" title="EIMTC.models.NearestNeighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> and <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the radius neighbors classifier from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted radius neighbors classifier.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.RadiusNeighborsClassifier" title="EIMTC.models.RadiusNeighborsClassifier">RadiusNeighborsClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict the class labels for the provided data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_queries</em><em>, </em><em>n_features</em><em>)</em><em>,                 or </em><em>(</em><em>n_queries</em><em>, </em><em>n_indexed</em><em>) </em><em>if metric == 'precomputed'</em>) – Test samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – Class labels for each data sample.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_queries,) or (n_queries, n_outputs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Return probability estimates for the test data X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_queries</em><em>, </em><em>n_features</em><em>)</em><em>,                 or </em><em>(</em><em>n_queries</em><em>, </em><em>n_indexed</em><em>) </em><em>if metric == 'precomputed'</em>) – Test samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – of such arrays if n_outputs &gt; 1.
The class probabilities of the input samples. Classes are ordered
by lexicographic order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_queries, n_classes), or a list of n_outputs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">RadiusNeighborsRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.RadiusNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Regression based on neighbors within a fixed radius.</p>
<p>The target is predicted by local interpolation of the targets
associated of the nearest neighbors in the training set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>radius</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Range of parameter space to use by default for <code class="xref py py-meth docutils literal notranslate"><span class="pre">radius_neighbors()</span></code>
queries.</p></li>
<li><p><strong>weights</strong> (<em>{'uniform'</em><em>, </em><em>'distance'}</em><em> or </em><em>callable</em><em>, </em><em>default='uniform'</em>) – <p>weight function used in prediction.  Possible values:</p>
<ul>
<li><p>’uniform’ : uniform weights.  All points in each neighborhood
are weighted equally.</p></li>
<li><p>’distance’ : weight points by the inverse of their distance.
in this case, closer neighbors of a query point will have a
greater influence than neighbors which are further away.</p></li>
<li><p>[callable] : a user-defined function which accepts an
array of distances, and returns an array of the same shape
containing the weights.</p></li>
</ul>
<p>Uniform weights are used by default.</p>
</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.RadiusNeighborsRegressor.fit" title="EIMTC.models.RadiusNeighborsRegressor.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Power parameter for the Minkowski metric. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – the distance metric to use for the tree.  The default metric is
minkowski, and with p=2 is equivalent to the standard Euclidean
metric. See the documentation of <a class="reference internal" href="#EIMTC.models.DistanceMetric" title="EIMTC.models.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a> for a
list of available metrics.
If metric is “precomputed”, X is assumed to be a distance matrix and
must be square during fit. X may be a <span class="xref std std-term">sparse graph</span>,
in which case only “nonzero” elements may be considered neighbors.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of parallel jobs to run for neighbors search.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsRegressor.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsRegressor.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The distance metric to use. It will be same as the <cite>metric</cite> parameter
or a synonym of it, e.g. ‘euclidean’ if the <cite>metric</cite> parameter set to
‘minkowski’ and <cite>p</cite> parameter set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str or callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsRegressor.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsRegressor.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Additional keyword arguments for the metric function. For most metrics
will be same with <cite>metric_params</cite> parameter, but may also contain the
<cite>p</cite> parameter value if the <cite>effective_metric_</cite> attribute is set to
‘minkowski’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsRegressor.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsRegressor.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="n">RadiusNeighborsRegressor</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RadiusNeighborsRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">neigh</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">]]))</span>
<span class="go">[0.5]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#EIMTC.models.NearestNeighbors" title="EIMTC.models.NearestNeighbors"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a>, <a class="reference internal" href="#EIMTC.models.KNeighborsRegressor" title="EIMTC.models.KNeighborsRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a>, <a class="reference internal" href="#EIMTC.models.KNeighborsClassifier" title="EIMTC.models.KNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a>, <a class="reference internal" href="#EIMTC.models.RadiusNeighborsClassifier" title="EIMTC.models.RadiusNeighborsClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>See <span class="xref std std-ref">Nearest Neighbors</span> in the online documentation
for a discussion of the choice of <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> and <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm">https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsRegressor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the radius neighbors regressor from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted radius neighbors regressor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.RadiusNeighborsRegressor" title="EIMTC.models.RadiusNeighborsRegressor">RadiusNeighborsRegressor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsRegressor.predict" title="Permalink to this definition"></a></dt>
<dd><p>Predict the target for the provided data</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_queries</em><em>, </em><em>n_features</em><em>)</em><em>,                 or </em><em>(</em><em>n_queries</em><em>, </em><em>n_indexed</em><em>) </em><em>if metric == 'precomputed'</em>) – Test samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – Target values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_queries,) or (n_queries, n_outputs),                 dtype=double</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">RadiusNeighborsTransformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'distance'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.RadiusNeighborsMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.TransformerMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._base.NeighborsBase</span></code></p>
<p>Transform X into a (weighted) graph of neighbors nearer than a radius</p>
<p>The transformed data is a sparse graph as returned by
radius_neighbors_graph.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<em>{'distance'</em><em>, </em><em>'connectivity'}</em><em>, </em><em>default='distance'</em>) – Type of returned matrix: ‘connectivity’ will return the connectivity
matrix with ones and zeros, and ‘distance’ will return the distances
between neighbors according to the given metric.</p></li>
<li><p><strong>radius</strong> (<em>float</em><em>, </em><em>default=1.</em>) – Radius of neighborhood in the transformed sparse graph.</p></li>
<li><p><strong>algorithm</strong> (<em>{'auto'</em><em>, </em><em>'ball_tree'</em><em>, </em><em>'kd_tree'</em><em>, </em><em>'brute'}</em><em>, </em><em>default='auto'</em>) – <p>Algorithm used to compute the nearest neighbors:</p>
<ul>
<li><p>’ball_tree’ will use <a class="reference internal" href="#EIMTC.models.BallTree" title="EIMTC.models.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a></p></li>
<li><p>’kd_tree’ will use <a class="reference internal" href="#EIMTC.models.KDTree" title="EIMTC.models.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a></p></li>
<li><p>’brute’ will use a brute-force search.</p></li>
<li><p>’auto’ will attempt to decide the most appropriate algorithm
based on the values passed to <a class="reference internal" href="#EIMTC.models.RadiusNeighborsTransformer.fit" title="EIMTC.models.RadiusNeighborsTransformer.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> method.</p></li>
</ul>
<p>Note: fitting on sparse input will override the setting of
this parameter, using brute force.</p>
</p></li>
<li><p><strong>leaf_size</strong> (<em>int</em><em>, </em><em>default=30</em>) – Leaf size passed to BallTree or KDTree.  This can affect the
speed of the construction and query, as well as the memory
required to store the tree.  The optimal value depends on the
nature of the problem.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='minkowski'</em>) – <p>metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>p</strong> (<em>int</em><em>, </em><em>default=2</em>) – Parameter for the Minkowski metric from
sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is
equivalent to using manhattan_distance (l1), and euclidean_distance
(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.</p></li>
<li><p><strong>metric_params</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Additional keyword arguments for the metric function.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=1</em>) – The number of parallel jobs to run for neighbors search.
If <code class="docutils literal notranslate"><span class="pre">-1</span></code>, then the number of jobs is set to the number of CPU cores.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer.effective_metric_">
<span class="sig-name descname"><span class="pre">effective_metric_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer.effective_metric_" title="Permalink to this definition"></a></dt>
<dd><p>The distance metric used. It will be same as the <cite>metric</cite> parameter
or a synonym of it, e.g. ‘euclidean’ if the <cite>metric</cite> parameter set to
‘minkowski’ and <cite>p</cite> parameter set to 2.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str or callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer.effective_metric_params_">
<span class="sig-name descname"><span class="pre">effective_metric_params_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer.effective_metric_params_" title="Permalink to this definition"></a></dt>
<dd><p>Additional keyword arguments for the metric function. For most metrics
will be same with <cite>metric_params</cite> parameter, but may also contain the
<cite>p</cite> parameter value if the <cite>effective_metric_</cite> attribute is set to
‘minkowski’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer.n_samples_fit_">
<span class="sig-name descname"><span class="pre">n_samples_fit_</span></span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer.n_samples_fit_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples in the fitted data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">RadiusNeighborsTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">RadiusNeighborsTransformer</span><span class="p">(</span><span class="n">radius</span><span class="o">=</span><span class="mf">42.0</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">DBSCAN</span><span class="p">(</span><span class="n">min_samples</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit the radius neighbors transformer from the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>) or                 </em><em>(</em><em>n_samples</em><em>, </em><em>n_samples</em><em>) </em><em>if metric='precomputed'</em>) – Training data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted radius neighbors transformer.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.RadiusNeighborsTransformer" title="EIMTC.models.RadiusNeighborsTransformer">RadiusNeighborsTransformer</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer.transform" title="Permalink to this definition"></a></dt>
<dd><p>Computes the (weighted) graph of Neighbors for points in X</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_transform</em><em>, </em><em>n_features</em><em>)</em>) – Sample data</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Xt</strong> – Xt[i, j] is assigned the weight of edge that connects i to j.
Only the neighbors have an explicit value.
The diagonal is always explicit.
The matrix is of CSR format.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples_transform, n_samples_fit)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.RadiusNeighborsTransformer.fit_transform">
<span class="sig-name descname"><span class="pre">fit_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.RadiusNeighborsTransformer.fit_transform" title="Permalink to this definition"></a></dt>
<dd><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training set.</p></li>
<li><p><strong>y</strong> (<em>ignored</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Xt</strong> – Xt[i, j] is assigned the weight of edge that connects i to j.
Only the neighbors have an explicit value.
The diagonal is always explicit.
The matrix is of CSR format.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_samples)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.KDTree">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">KDTree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.KDTree" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._kd_tree.BinaryTree</span></code></p>
<p>KDTree for fast generalized N-point problems</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – n_samples is the number of points in the data set, and
n_features is the dimension of the parameter space.
Note: if X is a C-contiguous array of doubles then data will
not be copied. Otherwise, an internal copy will be made.</p></li>
<li><p><strong>leaf_size</strong> (<em>positive int</em><em>, </em><em>default=40</em>) – Number of points at which to switch to brute-force. Changing
leaf_size will not affect the results of a query, but can
significantly impact the speed of a query and the memory required
to store the constructed tree.  The amount of memory needed to
store the tree scales as approximately n_samples / leaf_size.
For a specified <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>, a leaf node is guaranteed to
satisfy <code class="docutils literal notranslate"><span class="pre">leaf_size</span> <span class="pre">&lt;=</span> <span class="pre">n_points</span> <span class="pre">&lt;=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">leaf_size</span></code>, except in
the case that <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">&lt;</span> <span class="pre">leaf_size</span></code>.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>DistanceMetric object</em>) – the distance metric to use for the tree.  Default=’minkowski’
with p=2 (that is, a euclidean metric). See the documentation
of the DistanceMetric class for a list of available metrics.
kd_tree.valid_metrics gives a list of the metrics which
are valid for KDTree.</p></li>
<li><p><strong>class.</strong> (<em>Additional keywords are passed to the distance metric</em>) – </p></li>
<li><p><strong>Note</strong> (<em>Callable functions in the metric parameter are NOT supported for KDTree</em>) – </p></li>
<li><p><strong>performance.</strong> (<em>and Ball Tree. Function call overhead will result in very poor</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.KDTree.data">
<span class="sig-name descname"><span class="pre">data</span></span><a class="headerlink" href="#EIMTC.models.KDTree.data" title="Permalink to this definition"></a></dt>
<dd><p>The training data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>memory view</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<p>Query for k-nearest neighbors</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 10 points in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>              
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="p">,</span> <span class="n">ind</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>  <span class="c1"># indices of 3 closest neighbors</span>
<span class="go">[0 3 1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>  <span class="c1"># distances to 3 closest neighbors</span>
<span class="go">[ 0.          0.19662693  0.29473397]</span>
</pre></div>
</div>
<p>Pickle and Unpickle a tree.  Note that the state of the tree is saved in the
pickle operation: the tree needs not be rebuilt upon unpickling.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 10 points in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>        
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>                     
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree_copy</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="p">,</span> <span class="n">ind</span> <span class="o">=</span> <span class="n">tree_copy</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>     
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>  <span class="c1"># indices of 3 closest neighbors</span>
<span class="go">[0 3 1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>  <span class="c1"># distances to 3 closest neighbors</span>
<span class="go">[ 0.          0.19662693  0.29473397]</span>
</pre></div>
</div>
<p>Query for neighbors within a given radius</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 10 points in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>     
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">count_only</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ind</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>  <span class="c1"># indices of neighbors within distance 0.3</span>
<span class="go">[3 0 1]</span>
</pre></div>
</div>
<p>Compute a gaussian kernel density estimate:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">kernel_density</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">h</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">)</span>
<span class="go">array([ 6.94114649,  7.83281226,  7.2071716 ])</span>
</pre></div>
</div>
<p>Compute a two-point auto-correlation function</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">two_point_correlation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
<span class="go">array([ 30,  62, 278, 580, 820])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.BallTree">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">BallTree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaf_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'minkowski'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BallTree" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors._ball_tree.BinaryTree</span></code></p>
<p>BallTree for fast generalized N-point problems</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – n_samples is the number of points in the data set, and
n_features is the dimension of the parameter space.
Note: if X is a C-contiguous array of doubles then data will
not be copied. Otherwise, an internal copy will be made.</p></li>
<li><p><strong>leaf_size</strong> (<em>positive int</em><em>, </em><em>default=40</em>) – Number of points at which to switch to brute-force. Changing
leaf_size will not affect the results of a query, but can
significantly impact the speed of a query and the memory required
to store the constructed tree.  The amount of memory needed to
store the tree scales as approximately n_samples / leaf_size.
For a specified <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>, a leaf node is guaranteed to
satisfy <code class="docutils literal notranslate"><span class="pre">leaf_size</span> <span class="pre">&lt;=</span> <span class="pre">n_points</span> <span class="pre">&lt;=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">leaf_size</span></code>, except in
the case that <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">&lt;</span> <span class="pre">leaf_size</span></code>.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>DistanceMetric object</em>) – the distance metric to use for the tree.  Default=’minkowski’
with p=2 (that is, a euclidean metric). See the documentation
of the DistanceMetric class for a list of available metrics.
ball_tree.valid_metrics gives a list of the metrics which
are valid for BallTree.</p></li>
<li><p><strong>class.</strong> (<em>Additional keywords are passed to the distance metric</em>) – </p></li>
<li><p><strong>Note</strong> (<em>Callable functions in the metric parameter are NOT supported for KDTree</em>) – </p></li>
<li><p><strong>performance.</strong> (<em>and Ball Tree. Function call overhead will result in very poor</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BallTree.data">
<span class="sig-name descname"><span class="pre">data</span></span><a class="headerlink" href="#EIMTC.models.BallTree.data" title="Permalink to this definition"></a></dt>
<dd><p>The training data</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>memory view</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<p>Query for k-nearest neighbors</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 10 points in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">BallTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>              
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="p">,</span> <span class="n">ind</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>  <span class="c1"># indices of 3 closest neighbors</span>
<span class="go">[0 3 1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>  <span class="c1"># distances to 3 closest neighbors</span>
<span class="go">[ 0.          0.19662693  0.29473397]</span>
</pre></div>
</div>
<p>Pickle and Unpickle a tree.  Note that the state of the tree is saved in the
pickle operation: the tree needs not be rebuilt upon unpickling.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 10 points in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">BallTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>        
<span class="gp">&gt;&gt;&gt; </span><span class="n">s</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>                     
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree_copy</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="p">,</span> <span class="n">ind</span> <span class="o">=</span> <span class="n">tree_copy</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>     
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>  <span class="c1"># indices of 3 closest neighbors</span>
<span class="go">[0 3 1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>  <span class="c1"># distances to 3 closest neighbors</span>
<span class="go">[ 0.          0.19662693  0.29473397]</span>
</pre></div>
</div>
<p>Query for neighbors within a given radius</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>  <span class="c1"># 10 points in 3 dimensions</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">BallTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>     
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">count_only</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="go">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ind</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">query_radius</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">],</span> <span class="n">r</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>  <span class="c1"># indices of neighbors within distance 0.3</span>
<span class="go">[3 0 1]</span>
</pre></div>
</div>
<p>Compute a gaussian kernel density estimate:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">BallTree</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">kernel_density</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">3</span><span class="p">],</span> <span class="n">h</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">)</span>
<span class="go">array([ 6.94114649,  7.83281226,  7.2071716 ])</span>
</pre></div>
</div>
<p>Compute a two-point auto-correlation function</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random_sample</span><span class="p">((</span><span class="mi">30</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span> <span class="o">=</span> <span class="n">BallTree</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>                
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">two_point_correlation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
<span class="go">array([ 30,  62, 278, 580, 820])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.DistanceMetric">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">DistanceMetric</span></span><a class="headerlink" href="#EIMTC.models.DistanceMetric" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>DistanceMetric class</p>
<p>This class provides a uniform interface to fast distance metric
functions.  The various metrics can be accessed via the <a class="reference internal" href="#EIMTC.models.DistanceMetric.get_metric" title="EIMTC.models.DistanceMetric.get_metric"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_metric()</span></code></a>
class method and the metric string identifier (see below).</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">DistanceMetric</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span> <span class="o">=</span> <span class="n">DistanceMetric</span><span class="o">.</span><span class="n">get_metric</span><span class="p">(</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
<span class="go">         [3, 4, 5]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist</span><span class="o">.</span><span class="n">pairwise</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([[ 0.        ,  5.19615242],</span>
<span class="go">       [ 5.19615242,  0.        ]])</span>
</pre></div>
</div>
<p>Available Metrics</p>
<p>The following lists the string metric identifiers and the associated
distance metric classes:</p>
<p><strong>Metrics intended for real-valued vector spaces:</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 27%" />
<col style="width: 11%" />
<col style="width: 42%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>identifier</p></td>
<td><p>class name</p></td>
<td><p>args</p></td>
<td><p>distance function</p></td>
</tr>
<tr class="row-even"><td><p>“euclidean”</p></td>
<td><p>EuclideanDistance</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">sqrt(sum((x</span> <span class="pre">-</span> <span class="pre">y)^2))</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>“manhattan”</p></td>
<td><p>ManhattanDistance</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum(|x</span> <span class="pre">-</span> <span class="pre">y|)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>“chebyshev”</p></td>
<td><p>ChebyshevDistance</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">max(|x</span> <span class="pre">-</span> <span class="pre">y|)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>“minkowski”</p></td>
<td><p>MinkowskiDistance</p></td>
<td><p>p</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum(|x</span> <span class="pre">-</span> <span class="pre">y|^p)^(1/p)</span></code></p></td>
</tr>
<tr class="row-even"><td><p>“wminkowski”</p></td>
<td><p>WMinkowskiDistance</p></td>
<td><p>p, w</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum(|w</span> <span class="pre">*</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">y)|^p)^(1/p)</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>“seuclidean”</p></td>
<td><p>SEuclideanDistance</p></td>
<td><p>V</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sqrt(sum((x</span> <span class="pre">-</span> <span class="pre">y)^2</span> <span class="pre">/</span> <span class="pre">V))</span></code></p></td>
</tr>
<tr class="row-even"><td><p>“mahalanobis”</p></td>
<td><p>MahalanobisDistance</p></td>
<td><p>V or VI</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sqrt((x</span> <span class="pre">-</span> <span class="pre">y)'</span> <span class="pre">V^-1</span> <span class="pre">(x</span> <span class="pre">-</span> <span class="pre">y))</span></code></p></td>
</tr>
</tbody>
</table>
<p><strong>Metrics intended for two-dimensional vector spaces:</strong>  Note that the haversine
distance metric requires data in the form of [latitude, longitude] and both
inputs and outputs are in units of radians.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 13%" />
<col style="width: 19%" />
<col style="width: 68%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>identifier</p></td>
<td><p>class name</p></td>
<td><p>distance function</p></td>
</tr>
<tr class="row-even"><td><p>“haversine”</p></td>
<td><p>HaversineDistance</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">arcsin(sqrt(sin^2(0.5*dx)</span> <span class="pre">+</span> <span class="pre">cos(x1)cos(x2)sin^2(0.5*dy)))</span></code></p></td>
</tr>
</tbody>
</table>
<p><strong>Metrics intended for integer-valued vector spaces:</strong>  Though intended
for integer-valued vectors, these are also valid metrics in the case of
real-valued vectors.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 27%" />
<col style="width: 55%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>identifier</p></td>
<td><p>class name</p></td>
<td><p>distance function</p></td>
</tr>
<tr class="row-even"><td><p>“hamming”</p></td>
<td><p>HammingDistance</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">N_unequal(x,</span> <span class="pre">y)</span> <span class="pre">/</span> <span class="pre">N_tot</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>“canberra”</p></td>
<td><p>CanberraDistance</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum(|x</span> <span class="pre">-</span> <span class="pre">y|</span> <span class="pre">/</span> <span class="pre">(|x|</span> <span class="pre">+</span> <span class="pre">|y|))</span></code></p></td>
</tr>
<tr class="row-even"><td><p>“braycurtis”</p></td>
<td><p>BrayCurtisDistance</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">sum(|x</span> <span class="pre">-</span> <span class="pre">y|)</span> <span class="pre">/</span> <span class="pre">(sum(|x|)</span> <span class="pre">+</span> <span class="pre">sum(|y|))</span></code></p></td>
</tr>
</tbody>
</table>
<p><strong>Metrics intended for boolean-valued vector spaces:</strong>  Any nonzero entry
is evaluated to “True”.  In the listings below, the following
abbreviations are used:</p>
<blockquote>
<div><ul class="simple">
<li><p>N  : number of dimensions</p></li>
<li><p>NTT : number of dims in which both values are True</p></li>
<li><p>NTF : number of dims in which the first value is True, second is False</p></li>
<li><p>NFT : number of dims in which the first value is False, second is True</p></li>
<li><p>NFF : number of dims in which both values are False</p></li>
<li><p>NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT</p></li>
<li><p>NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT</p></li>
</ul>
</div></blockquote>
<table class="docutils align-default">
<colgroup>
<col style="width: 24%" />
<col style="width: 32%" />
<col style="width: 44%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>identifier</p></td>
<td><p>class name</p></td>
<td><p>distance function</p></td>
</tr>
<tr class="row-even"><td><p>“jaccard”</p></td>
<td><p>JaccardDistance</p></td>
<td><p>NNEQ / NNZ</p></td>
</tr>
<tr class="row-odd"><td><p>“matching”</p></td>
<td><p>MatchingDistance</p></td>
<td><p>NNEQ / N</p></td>
</tr>
<tr class="row-even"><td><p>“dice”</p></td>
<td><p>DiceDistance</p></td>
<td><p>NNEQ / (NTT + NNZ)</p></td>
</tr>
<tr class="row-odd"><td><p>“kulsinski”</p></td>
<td><p>KulsinskiDistance</p></td>
<td><p>(NNEQ + N - NTT) / (NNEQ + N)</p></td>
</tr>
<tr class="row-even"><td><p>“rogerstanimoto”</p></td>
<td><p>RogersTanimotoDistance</p></td>
<td><p>2 * NNEQ / (N + NNEQ)</p></td>
</tr>
<tr class="row-odd"><td><p>“russellrao”</p></td>
<td><p>RussellRaoDistance</p></td>
<td><p>(N - NTT) / N</p></td>
</tr>
<tr class="row-even"><td><p>“sokalmichener”</p></td>
<td><p>SokalMichenerDistance</p></td>
<td><p>2 * NNEQ / (N + NNEQ)</p></td>
</tr>
<tr class="row-odd"><td><p>“sokalsneath”</p></td>
<td><p>SokalSneathDistance</p></td>
<td><p>NNEQ / (NNEQ + 0.5 * NTT)</p></td>
</tr>
</tbody>
</table>
<p><strong>User-defined distance:</strong></p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 45%" />
<col style="width: 21%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>identifier</p></td>
<td><p>class name</p></td>
<td><p>args</p></td>
</tr>
<tr class="row-even"><td><p>“pyfunc”</p></td>
<td><p>PyFuncDistance</p></td>
<td><p>func</p></td>
</tr>
</tbody>
</table>
<p>Here <code class="docutils literal notranslate"><span class="pre">func</span></code> is a function which takes two one-dimensional numpy
arrays, and returns a distance.  Note that in order to be used within
the BallTree, the distance must be a true metric:
i.e. it must satisfy the following properties</p>
<ol class="arabic simple">
<li><p>Non-negativity: d(x, y) &gt;= 0</p></li>
<li><p>Identity: d(x, y) = 0 if and only if x == y</p></li>
<li><p>Symmetry: d(x, y) = d(y, x)</p></li>
<li><p>Triangle Inequality: d(x, y) + d(y, z) &gt;= d(x, z)</p></li>
</ol>
<p>Because of the Python object overhead involved in calling the python
function, this will be fairly slow, but it will have the same
scaling as other distances.</p>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DistanceMetric.dist_to_rdist">
<span class="sig-name descname"><span class="pre">dist_to_rdist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DistanceMetric.dist_to_rdist" title="Permalink to this definition"></a></dt>
<dd><p>Convert the true distance to the reduced distance.</p>
<p>The reduced distance, defined for some metrics, is a computationally
more efficient measure which preserves the rank of the true distance.
For example, in the Euclidean distance metric, the reduced distance
is the squared-euclidean distance.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DistanceMetric.get_metric">
<span class="sig-name descname"><span class="pre">get_metric</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DistanceMetric.get_metric" title="Permalink to this definition"></a></dt>
<dd><p>Get the given distance metric from the string identifier.</p>
<p>See the docstring of DistanceMetric for a list of available metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>metric</strong> (<em>string</em><em> or </em><em>class name</em>) – The distance metric to use</p></li>
<li><p><strong>**kwargs</strong> – additional arguments will be passed to the requested metric</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DistanceMetric.pairwise">
<span class="sig-name descname"><span class="pre">pairwise</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DistanceMetric.pairwise" title="Permalink to this definition"></a></dt>
<dd><p>Compute the pairwise distances between X and Y</p>
<p>This is a convenience routine for the sake of testing.  For many
metrics, the utilities in scipy.spatial.distance.cdist and
scipy.spatial.distance.pdist will be faster.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em>) – Array of shape (Nx, D), representing Nx points in D dimensions.</p></li>
<li><p><strong>Y</strong> (<em>array-like</em><em> (</em><em>optional</em><em>)</em>) – Array of shape (Ny, D), representing Ny points in D dimensions.
If not specified, then Y=X.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>dist</strong> – The shape (Nx, Ny) array of pairwise distances between points in
X and Y.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DistanceMetric.rdist_to_dist">
<span class="sig-name descname"><span class="pre">rdist_to_dist</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DistanceMetric.rdist_to_dist" title="Permalink to this definition"></a></dt>
<dd><p>Convert the Reduced distance to the true distance.</p>
<p>The reduced distance, defined for some metrics, is a computationally
more efficient measure which preserves the rank of the true distance.
For example, in the Euclidean distance metric, the reduced distance
is the squared-euclidean distance.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">BernoulliNB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">binarize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.BernoulliNB" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes._BaseDiscreteNB</span></code></p>
<p>Naive Bayes classifier for multivariate Bernoulli models.</p>
<p>Like MultinomialNB, this classifier is suitable for discrete data. The
difference is that while MultinomialNB works with occurrence counts,
BernoulliNB is designed for binary/boolean features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</p></li>
<li><p><strong>binarize</strong> (<em>float</em><em> or </em><em>None</em><em>, </em><em>default=0.0</em>) – Threshold for binarizing (mapping to booleans) of sample features.
If None, input is presumed to already consist of binary vectors.</p></li>
<li><p><strong>fit_prior</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</p></li>
<li><p><strong>class_prior</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.class_count_">
<span class="sig-name descname"><span class="pre">class_count_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.class_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.class_log_prior_">
<span class="sig-name descname"><span class="pre">class_log_prior_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.class_log_prior_" title="Permalink to this definition"></a></dt>
<dd><p>Log probability of each class (smoothed).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels known to the classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Mirrors <code class="docutils literal notranslate"><span class="pre">feature_log_prob_</span></code> for interpreting <cite>BernoulliNB</cite>
as a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.feature_count_">
<span class="sig-name descname"><span class="pre">feature_count_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.feature_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.feature_log_prob_">
<span class="sig-name descname"><span class="pre">feature_log_prob_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.feature_log_prob_" title="Permalink to this definition"></a></dt>
<dd><p>Empirical log probability of features given a class, P(x_i|y).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Mirrors <code class="docutils literal notranslate"><span class="pre">class_log_prior_</span></code> for interpreting <cite>BernoulliNB</cite>
as a linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.BernoulliNB.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.BernoulliNB.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>Number of features of each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">BernoulliNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BernoulliNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">BernoulliNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p class="rubric">References</p>
<p>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
<a class="reference external" href="https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html</a></p>
<p>A. McCallum and K. Nigam (1998). A comparison of event models for naive
Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for
Text Categorization, pp. 41-48.</p>
<p>V. Metsis, I. Androutsopoulos and G. Paliouras (2006). Spam filtering with
naive Bayes – Which naive Bayes? 3rd Conf. on Email and Anti-Spam (CEAS).</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">CategoricalNB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_categories</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CategoricalNB" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes._BaseDiscreteNB</span></code></p>
<p>Naive Bayes classifier for categorical features</p>
<p>The categorical Naive Bayes classifier is suitable for classification with
discrete features that are categorically distributed. The categories of
each feature are drawn from a categorical distribution.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</p></li>
<li><p><strong>fit_prior</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</p></li>
<li><p><strong>class_prior</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</p></li>
<li><p><strong>min_categories</strong> (<em>int</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Minimum number of categories per feature.</p>
<ul>
<li><p>integer: Sets the minimum number of categories per feature to
<cite>n_categories</cite> for each features.</p></li>
<li><p>array-like: shape (n_features,) where <cite>n_categories[i]</cite> holds the
minimum number of categories for the ith column of the input.</p></li>
<li><p>None (default): Determines the number of categories automatically
from the training data.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.category_count_">
<span class="sig-name descname"><span class="pre">category_count_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.category_count_" title="Permalink to this definition"></a></dt>
<dd><p>Holds arrays of shape (n_classes, n_categories of respective feature)
for each feature. Each array provides the number of samples
encountered for each class and category of the specific feature.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.class_count_">
<span class="sig-name descname"><span class="pre">class_count_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.class_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.class_log_prior_">
<span class="sig-name descname"><span class="pre">class_log_prior_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.class_log_prior_" title="Permalink to this definition"></a></dt>
<dd><p>Smoothed empirical log probability for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels known to the classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.feature_log_prob_">
<span class="sig-name descname"><span class="pre">feature_log_prob_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.feature_log_prob_" title="Permalink to this definition"></a></dt>
<dd><p>Holds arrays of shape (n_classes, n_categories of respective feature)
for each feature. Each array provides the empirical log probability
of categories given the respective feature and class, <code class="docutils literal notranslate"><span class="pre">P(x_i|y)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>Number of features of each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.n_categories_">
<span class="sig-name descname"><span class="pre">n_categories_</span></span><a class="headerlink" href="#EIMTC.models.CategoricalNB.n_categories_" title="Permalink to this definition"></a></dt>
<dd><p>Number of categories for each feature. This value is
inferred from the data or set by the minimum number of categories.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,), dtype=np.int64</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">CategoricalNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">CategoricalNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">CategoricalNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CategoricalNB.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit Naive Bayes classifier according to X, y</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features. Here, each feature of X is
assumed to be from a different categorical distribution.
It is further assumed that all categories of each feature are
represented by the numbers 0, …, n - 1, where n refers to the
total number of categories for the given feature. This can, for
instance, be achieved with the help of OrdinalEncoder.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>)</em><em>, </em><em>default=None</em>) – Weights applied to individual samples (1. for unweighted).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.CategoricalNB.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.CategoricalNB.partial_fit" title="Permalink to this definition"></a></dt>
<dd><p>Incremental fit on a batch of samples.</p>
<p>This method is expected to be called several times consecutively
on different chunks of a dataset so as to implement out-of-core
or online learning.</p>
<p>This is especially useful when the whole dataset is too big to fit in
memory at once.</p>
<p>This method has some performance overhead hence it is better to call
partial_fit on chunks of data that are as large as possible
(as long as fitting in the memory budget) to hide the overhead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features. Here, each feature of X is
assumed to be from a different categorical distribution.
It is further assumed that all categories of each feature are
represented by the numbers 0, …, n - 1, where n refers to the
total number of categories for the given feature. This can, for
instance, be achieved with the help of OrdinalEncoder.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>)</em>) – Target values.</p></li>
<li><p><strong>classes</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>)</em><em>, </em><em>default=None</em>) – <p>List of all the classes that can possibly appear in the y vector.</p>
<p>Must be provided at the first call to partial_fit, can be omitted
in subsequent calls.</p>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>)</em><em>, </em><em>default=None</em>) – Weights applied to individual samples (1. for unweighted).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">ComplementNB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ComplementNB" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes._BaseDiscreteNB</span></code></p>
<p>The Complement Naive Bayes classifier described in Rennie et al. (2003).</p>
<p>The Complement Naive Bayes classifier was designed to correct the “severe
assumptions” made by the standard Multinomial Naive Bayes classifier. It is
particularly suited for imbalanced data sets.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).</p></li>
<li><p><strong>fit_prior</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Only used in edge case with a single class in the training set.</p></li>
<li><p><strong>class_prior</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Prior probabilities of the classes. Not used.</p></li>
<li><p><strong>norm</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether or not a second normalization of the weights is performed. The
default behavior mirrors the implementations found in Mahout and Weka,
which do not follow the full algorithm described in Table 9 of the
paper.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.class_count_">
<span class="sig-name descname"><span class="pre">class_count_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.class_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.class_log_prior_">
<span class="sig-name descname"><span class="pre">class_log_prior_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.class_log_prior_" title="Permalink to this definition"></a></dt>
<dd><p>Smoothed empirical log probability for each class. Only used in edge
case with a single class in the training set.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels known to the classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Mirrors <code class="docutils literal notranslate"><span class="pre">feature_log_prob_</span></code> for interpreting <cite>ComplementNB</cite>
as a linear model.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><code class="docutils literal notranslate"><span class="pre">coef_</span></code> is deprecated in 0.24 and will be removed in 1.1
(renaming of 0.26).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.feature_all_">
<span class="sig-name descname"><span class="pre">feature_all_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.feature_all_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each feature during fitting. This
value is weighted by the sample weight when provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.feature_count_">
<span class="sig-name descname"><span class="pre">feature_count_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.feature_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each (class, feature) during fitting.
This value is weighted by the sample weight when provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.feature_log_prob_">
<span class="sig-name descname"><span class="pre">feature_log_prob_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.feature_log_prob_" title="Permalink to this definition"></a></dt>
<dd><p>Empirical weights for class complements.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Mirrors <code class="docutils literal notranslate"><span class="pre">class_log_prior_</span></code> for interpreting <cite>ComplementNB</cite>
as a linear model.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><code class="docutils literal notranslate"><span class="pre">coef_</span></code> is deprecated in 0.24 and will be removed in 1.1
(renaming of 0.26).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ComplementNB.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.ComplementNB.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>Number of features of each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">ComplementNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ComplementNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ComplementNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p class="rubric">References</p>
<p>Rennie, J. D., Shih, L., Teevan, J., &amp; Karger, D. R. (2003).
Tackling the poor assumptions of naive bayes text classifiers. In ICML
(Vol. 3, pp. 616-623).
<a class="reference external" href="https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf">https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">GaussianNB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">priors</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">var_smoothing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-09</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GaussianNB" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes._BaseNB</span></code></p>
<p>Gaussian Naive Bayes (GaussianNB)</p>
<p>Can perform online updates to model parameters via <a class="reference internal" href="#EIMTC.models.GaussianNB.partial_fit" title="EIMTC.models.GaussianNB.partial_fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit()</span></code></a>.
For details on algorithm used to update feature means and variance online,
see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:</p>
<blockquote>
<div><p><a class="reference external" href="http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf">http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf</a></p>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>priors</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em>) – Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</p></li>
<li><p><strong>var_smoothing</strong> (<em>float</em><em>, </em><em>default=1e-9</em>) – <p>Portion of the largest variance of all features that is added to
variances for calculation stability.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.class_count_">
<span class="sig-name descname"><span class="pre">class_count_</span></span><a class="headerlink" href="#EIMTC.models.GaussianNB.class_count_" title="Permalink to this definition"></a></dt>
<dd><p>number of training samples observed in each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.class_prior_">
<span class="sig-name descname"><span class="pre">class_prior_</span></span><a class="headerlink" href="#EIMTC.models.GaussianNB.class_prior_" title="Permalink to this definition"></a></dt>
<dd><p>probability of each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.GaussianNB.classes_" title="Permalink to this definition"></a></dt>
<dd><p>class labels known to the classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.epsilon_">
<span class="sig-name descname"><span class="pre">epsilon_</span></span><a class="headerlink" href="#EIMTC.models.GaussianNB.epsilon_" title="Permalink to this definition"></a></dt>
<dd><p>absolute additive value to variances</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.sigma_">
<span class="sig-name descname"><span class="pre">sigma_</span></span><a class="headerlink" href="#EIMTC.models.GaussianNB.sigma_" title="Permalink to this definition"></a></dt>
<dd><p>variance of each feature per class</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.theta_">
<span class="sig-name descname"><span class="pre">theta_</span></span><a class="headerlink" href="#EIMTC.models.GaussianNB.theta_" title="Permalink to this definition"></a></dt>
<dd><p>mean of each feature per class</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">GaussianNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf_pf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf_pf</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y</span><span class="p">))</span>
<span class="go">GaussianNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf_pf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GaussianNB.fit" title="Permalink to this definition"></a></dt>
<dd><p>Fit Gaussian Naive Bayes according to X, y</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples
and n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Weights applied to individual samples (1. for unweighted).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>Gaussian Naive Bayes supports fitting with <em>sample_weight</em>.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.GaussianNB.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.GaussianNB.partial_fit" title="Permalink to this definition"></a></dt>
<dd><p>Incremental fit on a batch of samples.</p>
<p>This method is expected to be called several times consecutively
on different chunks of a dataset so as to implement out-of-core
or online learning.</p>
<p>This is especially useful when the whole dataset is too big to fit in
memory at once.</p>
<p>This method has some performance and numerical stability overhead,
hence it is better to call partial_fit on chunks of data that are
as large as possible (as long as fitting in the memory budget) to
hide the overhead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>classes</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>List of all the classes that can possibly appear in the y vector.</p>
<p>Must be provided at the first call to partial_fit, can be omitted
in subsequent calls.</p>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Weights applied to individual samples (1. for unweighted).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">MultinomialNB</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_prior</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.MultinomialNB" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.naive_bayes._BaseDiscreteNB</span></code></p>
<p>Naive Bayes classifier for multinomial models</p>
<p>The multinomial Naive Bayes classifier is suitable for classification with
discrete features (e.g., word counts for text classification). The
multinomial distribution normally requires integer feature counts. However,
in practice, fractional counts such as tf-idf may also work.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Additive (Laplace/Lidstone) smoothing parameter
(0 for no smoothing).</p></li>
<li><p><strong>fit_prior</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to learn class prior probabilities or not.
If false, a uniform prior will be used.</p></li>
<li><p><strong>class_prior</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Prior probabilities of the classes. If specified the priors are not
adjusted according to the data.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.class_count_">
<span class="sig-name descname"><span class="pre">class_count_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.class_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each class during fitting. This
value is weighted by the sample weight when provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.class_log_prior_">
<span class="sig-name descname"><span class="pre">class_log_prior_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.class_log_prior_" title="Permalink to this definition"></a></dt>
<dd><p>Smoothed empirical log probability for each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.classes_" title="Permalink to this definition"></a></dt>
<dd><p>Class labels known to the classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.coef_" title="Permalink to this definition"></a></dt>
<dd><p>Mirrors <code class="docutils literal notranslate"><span class="pre">feature_log_prob_</span></code> for interpreting <cite>MultinomialNB</cite>
as a linear model.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><code class="docutils literal notranslate"><span class="pre">coef_</span></code> is deprecated in 0.24 and will be removed in 1.1
(renaming of 0.26).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.feature_count_">
<span class="sig-name descname"><span class="pre">feature_count_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.feature_count_" title="Permalink to this definition"></a></dt>
<dd><p>Number of samples encountered for each (class, feature)
during fitting. This value is weighted by the sample weight when
provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.feature_log_prob_">
<span class="sig-name descname"><span class="pre">feature_log_prob_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.feature_log_prob_" title="Permalink to this definition"></a></dt>
<dd><p>Empirical log probability of features
given a class, <code class="docutils literal notranslate"><span class="pre">P(x_i|y)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.intercept_" title="Permalink to this definition"></a></dt>
<dd><p>Mirrors <code class="docutils literal notranslate"><span class="pre">class_log_prior_</span></code> for interpreting <cite>MultinomialNB</cite>
as a linear model.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><code class="docutils literal notranslate"><span class="pre">intercept_</span></code> is deprecated in 0.24 and will be removed in 1.1
(renaming of 0.26).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.MultinomialNB.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.MultinomialNB.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>Number of features of each sample.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">MultinomialNB()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">3</span><span class="p">]))</span>
<span class="go">[3]</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>For the rationale behind the names <cite>coef_</cite> and <cite>intercept_</cite>, i.e.
naive Bayes as a linear classifier, see J. Rennie et al. (2003),
Tackling the poor assumptions of naive Bayes text classifiers, ICML.</p>
<p class="rubric">References</p>
<p>C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
<a class="reference external" href="https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">DecisionTreeClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.BaseDecisionTree</span></code></p>
<p>A decision tree classifier.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;best&quot;</em><em>, </em><em>&quot;random&quot;}</em><em>, </em><em>default=&quot;best&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em> or </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
</div></blockquote>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the randomness of the estimator. The features are always
randomly permuted at each split, even if <code class="docutils literal notranslate"><span class="pre">splitter</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">&quot;best&quot;</span></code>. When <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>, the algorithm will
select <code class="docutils literal notranslate"><span class="pre">max_features</span></code> at random at each split before finding the best
split among them. But the best found split may vary across different
runs, even if <code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>. That is the case, if the
improvement of the criterion is identical for several splits and one
split has to be selected at random. To obtain a deterministic behaviour
during fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed to an integer.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=0</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>list of dict</em><em> or </em><em>&quot;balanced&quot;</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If None, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance <a href="#id86"><span class="problematic" id="id60">[4]_</span></a>.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.max_features_" title="Permalink to this definition"></a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.tree_" title="Permalink to this definition"></a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.DecisionTreeRegressor" title="EIMTC.models.DecisionTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a></dt><dd><p>A decision tree regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code> method operates using the <code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.argmax()</span></code>
function on the outputs of <a class="reference internal" href="#EIMTC.models.DecisionTreeClassifier.predict_proba" title="EIMTC.models.DecisionTreeClassifier.predict_proba"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_proba()</span></code></a>. This means that in
case the highest predicted probabilities are tied, the classifier will
predict the tied class with the lowest index in <span class="xref std std-term">classes_</span>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id61"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></p>
</dd>
<dt class="label" id="id62"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and Regression Trees”, Wadsworth, Belmont, CA, 1984.</p>
</dd>
<dt class="label" id="id63"><span class="brackets">3</span></dt>
<dd><p>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical
Learning”, Springer, 2009.</p>
</dd>
<dt class="label" id="id64"><span class="brackets">4</span></dt>
<dd><p>L. Breiman, and A. Cutler, “Random Forests”,
<a class="reference external" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="gp">...</span>
<span class="go">array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,</span>
<span class="go">        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.fit" title="Permalink to this definition"></a></dt>
<dd><p>Build a decision tree classifier from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – The target values (class labels) as integers or strings.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. Splits are also
ignored if they would result in any single class carrying a
negative weight in either child node.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
<li><p><strong>X_idx_sorted</strong> (<em>deprecated</em><em>, </em><em>default=&quot;deprecated&quot;</em>) – <p>This parameter is deprecated and has no effect.
It will be removed in 1.1 (renaming of 0.26).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.DecisionTreeClassifier" title="EIMTC.models.DecisionTreeClassifier">DecisionTreeClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.predict_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class probabilities of the input samples X.</p>
<p>The predicted class probability is the fraction of samples of the same
class in a leaf.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>proba</strong> – The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &gt; 1</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DecisionTreeClassifier.predict_log_proba" title="Permalink to this definition"></a></dt>
<dd><p>Predict class log-probabilities of the input samples X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>proba</strong> – The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &gt; 1</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">DecisionTreeRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.BaseDecisionTree</span></code></p>
<p>A decision tree regressor.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;mse&quot;</em><em>, </em><em>&quot;friedman_mse&quot;</em><em>, </em><em>&quot;mae&quot;</em><em>, </em><em>&quot;poisson&quot;}</em><em>, </em><em>default=&quot;mse&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion and minimizes the L2 loss
using the mean of each terminal node, “friedman_mse”, which uses mean
squared error with Friedman’s improvement score for potential splits,
“mae” for the mean absolute error, which minimizes the L1 loss using
the median of each terminal node, and “poisson” which uses reduction in
Poisson deviance to find splits.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24: </span>Poisson deviance criterion.</p>
</div>
</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;best&quot;</em><em>, </em><em>&quot;random&quot;}</em><em>, </em><em>default=&quot;best&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em> or </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the randomness of the estimator. The features are always
randomly permuted at each split, even if <code class="docutils literal notranslate"><span class="pre">splitter</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">&quot;best&quot;</span></code>. When <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>, the algorithm will
select <code class="docutils literal notranslate"><span class="pre">max_features</span></code> at random at each split before finding the best
split among them. But the best found split may vary across different
runs, even if <code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>. That is the case, if the
improvement of the criterion is identical for several splits and one
split has to be selected at random. To obtain a deterministic behaviour
during fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed to an integer.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=0</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the
(normalized) total reduction of the criterion brought
by that feature. It is also known as the Gini importance <a href="#id87"><span class="problematic" id="id65">[4]_</span></a>.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor.max_features_" title="Permalink to this definition"></a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor.tree_" title="Permalink to this definition"></a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.DecisionTreeClassifier" title="EIMTC.models.DecisionTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a></dt><dd><p>A decision tree classifier.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id66"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></p>
</dd>
<dt class="label" id="id67"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and Regression Trees”, Wadsworth, Belmont, CA, 1984.</p>
</dd>
<dt class="label" id="id68"><span class="brackets">3</span></dt>
<dd><p>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical
Learning”, Springer, 2009.</p>
</dd>
<dt class="label" id="id69"><span class="brackets">4</span></dt>
<dd><p>L. Breiman, and A. Cutler, “Random Forests”,
<a class="reference external" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                   
<span class="gp">...</span>
<span class="go">array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,</span>
<span class="go">       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="EIMTC.models.DecisionTreeRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.DecisionTreeRegressor.fit" title="Permalink to this definition"></a></dt>
<dd><p>Build a decision tree regressor from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – The target values (real numbers). Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float64</span></code> and
<code class="docutils literal notranslate"><span class="pre">order='C'</span></code> for maximum efficiency.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
<li><p><strong>X_idx_sorted</strong> (<em>deprecated</em><em>, </em><em>default=&quot;deprecated&quot;</em>) – <p>This parameter is deprecated and has no effect.
It will be removed in 1.1 (renaming of 0.26).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#EIMTC.models.DecisionTreeRegressor" title="EIMTC.models.DecisionTreeRegressor">DecisionTreeRegressor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">ExtraTreeClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#EIMTC.models.DecisionTreeClassifier" title="sklearn.tree._classes.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.DecisionTreeClassifier</span></code></a></p>
<p>An extremely randomized tree classifier.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;random&quot;</em><em>, </em><em>&quot;best&quot;}</em><em>, </em><em>default=&quot;random&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em>, </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em> or </em><em>None</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
</div></blockquote>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Used to pick randomly the <cite>max_features</cite> used at each split.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>list of dict</em><em> or </em><em>&quot;balanced&quot;</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If None, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.classes_" title="Permalink to this definition"></a></dt>
<dd><p>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.max_features_" title="Permalink to this definition"></a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.n_classes_" title="Permalink to this definition"></a></dt>
<dd><p>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeClassifier.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeClassifier.tree_" title="Permalink to this definition"></a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.ExtraTreeRegressor" title="EIMTC.models.ExtraTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreeRegressor</span></code></a></dt><dd><p>An extremely randomized tree regressor.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesClassifier</span></code></dt><dd><p>An extra-trees classifier.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesRegressor</span></code></dt><dd><p>An extra-trees regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id70"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">ExtraTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extra_tree</span> <span class="o">=</span> <span class="n">ExtraTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">cls</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">extra_tree</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">cls</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.8947...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeRegressor">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">EIMTC.models.</span></span><span class="sig-name descname"><span class="pre">ExtraTreeRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#EIMTC.models.ExtraTreeRegressor" title="Permalink to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#EIMTC.models.DecisionTreeRegressor" title="sklearn.tree._classes.DecisionTreeRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.DecisionTreeRegressor</span></code></a></p>
<p>An extremely randomized tree regressor.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;mse&quot;</em><em>, </em><em>&quot;friedman_mse&quot;</em><em>, </em><em>&quot;mae&quot;}</em><em>, </em><em>default=&quot;mse&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “mse” for the mean squared error, which is equal to variance
reduction as feature selection criterion and “mae” for the mean
absolute error.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24: </span>Poisson deviance criterion.</p>
</div>
</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;random&quot;</em><em>, </em><em>&quot;best&quot;}</em><em>, </em><em>default=&quot;random&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em>, </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em> or </em><em>None</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Used to pick randomly the <cite>max_features</cite> used at each split.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeRegressor.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeRegressor.max_features_" title="Permalink to this definition"></a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeRegressor.n_features_" title="Permalink to this definition"></a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeRegressor.feature_importances_" title="Permalink to this definition"></a></dt>
<dd><p>Return impurity-based feature importances (the higher, the more
important the feature).</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeRegressor.n_outputs_" title="Permalink to this definition"></a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="EIMTC.models.ExtraTreeRegressor.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#EIMTC.models.ExtraTreeRegressor.tree_" title="Permalink to this definition"></a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#EIMTC.models.ExtraTreeClassifier" title="EIMTC.models.ExtraTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreeClassifier</span></code></a></dt><dd><p>An extremely randomized tree classifier.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesClassifier</span></code></dt><dd><p>An extra-trees classifier.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesRegressor</span></code></dt><dd><p>An extra-trees regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id71"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">ExtraTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extra_tree</span> <span class="o">=</span> <span class="n">ExtraTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">extra_tree</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.33...</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="EIMTC.metrics.html" class="btn btn-neutral float-left" title="EIMTC.metrics package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="EIMTC.plugins.html" class="btn btn-neutral float-right" title="EIMTC.plugins package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Ariel University.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>